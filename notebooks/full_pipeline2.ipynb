{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a6dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime, timezone\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format for the log messages\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Log to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from core_data_prep.core_data_prep import DataPreparer\n",
    "from core_data_prep.validations import Validator\n",
    "\n",
    "from data.raw.retrievers.alpaca_markets_retriever import AlpacaMarketsRetriever\n",
    "from data.raw.retrievers.stooq_retriever import StooqRetriever\n",
    "from config.constants import *\n",
    "from data.processed.dataset_creation import DatasetCreator\n",
    "from data.processed.indicators import *\n",
    "from data.processed.targets import Balanced3ClassClassification\n",
    "from data.processed.normalization import ZScoreOverWindowNormalizer, ZScoreNormalizer, MinMaxNormalizer\n",
    "from data.processed.dataset_pytorch import DatasetPytorch\n",
    "from modeling.trainer import Trainer\n",
    "from modeling.evaluate import evaluate_lgb_regressor, evaluate_torch_regressor, evaluate_torch_regressor_multiasset\n",
    "from modeling.modeling_utils import print_model_parameters\n",
    "\n",
    "from modeling.rl.environment import PortfolioEnvironment\n",
    "from modeling.rl.state import State\n",
    "from modeling.rl.agent import RlAgent\n",
    "from modeling.rl.algorithms.policy_gradient import PolicyGradient\n",
    "from modeling.rl.actors.actor import RlActor\n",
    "from modeling.rl.actors.signal_predictor_actor import SignalPredictorActor\n",
    "from modeling.rl.actors.high_energy_low_friction_actor import HighEnergyLowFrictionActor\n",
    "from modeling.rl.actors.xsmom_actor import XSMomActor\n",
    "from modeling.rl.actors.tsmom_actor import TSMomActor\n",
    "from modeling.rl.actors.blsw_actor import BLSWActor\n",
    "from modeling.rl.actors.allocation_propogation_actor import AllocationPropogationActor\n",
    "from modeling.rl.actors.market_actor import MarketActor\n",
    "from modeling.rl.trajectory_dataset import TrajectoryDataset\n",
    "from modeling.rl.metrics import MetricsCalculator, DEFAULT_METRICS\n",
    "from modeling.rl.reward import EstimatedReturnReward\n",
    "from modeling.rl.loss import SumLogReturnLoss, ReinforceLoss\n",
    "from modeling.rl.visualization.wealth_plot import plot_cumulative_wealth\n",
    "from modeling.rl.visualization.position_plot import plot_position_heatmap\n",
    "from config.experiments.cur_experiment import config\n",
    "\n",
    "torch.backends.cudnn.benchmark = config.train_config.cudnn_benchmark\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1e9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = config.data_config.retriever\n",
    "retrieval_result = retriever.bars_with_quotes(\n",
    "    symbol_or_symbols=config.data_config.symbol_or_symbols, \n",
    "    start=config.data_config.start, \n",
    "    end=config.data_config.end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39000dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparer = DataPreparer(\n",
    "    normalizer=config.data_config.normalizer,\n",
    "    missing_values_handler=config.data_config.missing_values_handler_polars,\n",
    "    in_seq_len=config.data_config.in_seq_len,\n",
    "    frequency=str(config.data_config.frequency),\n",
    "    validator=config.data_config.validator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2313bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 12:08:51,102 - INFO - Skipping day 2024-09-03 00:00:00-04:00 because it has less than 50 assets\n",
      "2026-01-05 12:08:51,229 - INFO - Skipping day 2024-09-04 00:00:00-04:00 because it has less than 50 assets\n",
      "2026-01-05 12:09:34,910 - INFO - Found 280 daily slices\n",
      "2026-01-05 12:09:34,936 - INFO - Trained per-asset targets\n",
      "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((83674, 50, 60, 16),\n",
       " (83674, 50),\n",
       " (83674, 50),\n",
       " (8993, 50, 60, 16),\n",
       " (8993, 50),\n",
       " (8993, 50),\n",
       " (16813, 50, 60, 16),\n",
       " (16813, 50),\n",
       " (16813, 50))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train, statistics_train), (X_val, y_val, statistics_val), (X_test, y_test, statistics_test) = \\\n",
    "    data_preparer.get_experiment_data(\n",
    "        data=retrieval_result,\n",
    "        start_date=config.data_config.start,\n",
    "        end_date=config.data_config.end,\n",
    "        features=config.data_config.features_polars,\n",
    "        statistics=config.data_config.statistics,\n",
    "        target=config.data_config.target,\n",
    "        train_set_last_date=config.data_config.train_set_last_date,\n",
    "        val_set_last_date=config.data_config.val_set_last_date,\n",
    "        backend='loky'\n",
    "    )\n",
    "\n",
    "X_train.shape, y_train.shape, statistics_train['next_return'].shape, \\\n",
    "    X_val.shape, y_val.shape, statistics_val['next_return'].shape, \\\n",
    "    X_test.shape, y_test.shape, statistics_test['next_return'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a313bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from observability.mlflow_integration import log_experiment\n",
    "\n",
    "\n",
    "# log_experiment(\n",
    "#     config=config, \n",
    "#     validator_snapshots=data_preparer.validator.snapshots\n",
    "#     # model=model, \n",
    "#     # history=history,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca6e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_return_train, spread_train, volatility_train, \\\n",
    "    next_return_val, spread_val, volatility_val, \\\n",
    "    next_return_test, spread_test, volatility_test = \\\n",
    "        statistics_train['next_return'], statistics_train['spread'], statistics_train['volatility'], \\\n",
    "        statistics_val['next_return'], statistics_val['spread'], statistics_val['volatility'], \\\n",
    "        statistics_test['next_return'], statistics_test['spread'], statistics_test['volatility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e29a11c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00068538164, 0.0002124158, 0.00086798106)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(next_return_train).mean(), spread_train.mean(), volatility_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d798023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0004901743, 0.00021241582, 0.00062929775)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(next_return_val).mean(), spread_val.mean(), volatility_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677eea8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00051087514, 0.00021241585, 0.0006510256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(next_return_test).mean(), spread_test.mean(), volatility_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b98a4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetPytorch(X_train, y_train, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")\n",
    "val_loader = DatasetPytorch(X_val, y_val, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c4b5aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalSpatial(\n",
       "  (asset_embed): Embedding(50, 16)\n",
       "  (asset_proj): Linear(in_features=16, out_features=256, bias=False)\n",
       "  (lstm): LSTM(16, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (spatial_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = config.model_config.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b056846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                                   Params\n",
      "------------------------------------------------------------\n",
      "[ROOT]                                   813601\n",
      "asset_embed                              800\n",
      "asset_proj                               4096\n",
      "lstm                                     544768\n",
      "spatial_attn                             263168\n",
      "spatial_attn.out_proj                    65792\n",
      "fc                                       257\n",
      "norm                                     512\n"
     ]
    }
   ],
   "source": [
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa26ca6-8128-4e85-be01-34335ab51675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentConfig(data_config=DataConfig(retriever=<data.raw.retrievers.alpaca_markets_retriever.AlpacaMarketsRetriever object at 0x7a7e6eda1510>, symbol_or_symbols=['AAPL', 'AMD', 'BABA', 'BITU', 'C', 'CSCO', 'DAL', 'DIA', 'GLD', 'GOOG', 'IJR', 'MARA', 'MRVL', 'MU', 'NEE', 'NKE', 'NVDA', 'ON', 'PLTR', 'PYPL', 'QLD', 'QQQ', 'QQQM', 'RKLB', 'RSP', 'SMCI', 'SMH', 'SOXL', 'SOXX', 'SPXL', 'SPY', 'TMF', 'TNA', 'TQQQ', 'TSLA', 'UBER', 'UDOW', 'UPRO', 'VOO', 'WFC', 'XBI', 'XLC', 'XLE', 'XLI', 'XLK', 'XLU', 'XLV', 'XLY', 'XOM', 'XRT'], frequency=<alpaca.data.timeframe.TimeFrame object at 0x7a7e6ecc14d0>, start=datetime.datetime(2024, 9, 1, 0, 0, tzinfo=zoneinfo.ZoneInfo(key='America/New_York')), end=datetime.datetime(2025, 10, 1, 0, 0, tzinfo=zoneinfo.ZoneInfo(key='America/New_York')), train_set_last_date=datetime.datetime(2025, 7, 1, 0, 0, tzinfo=zoneinfo.ZoneInfo(key='America/New_York')), val_set_last_date=datetime.datetime(2025, 8, 1, 0, 0, tzinfo=zoneinfo.ZoneInfo(key='America/New_York')), features_polars={'log_ret': <function <lambda> at 0x7a7f69bfc400>, 'hl_range': <function <lambda> at 0x7a7e79f72660>, 'close_open': <function <lambda> at 0x7a8017b8f420>, 'vol_delta': <function <lambda> at 0x7a7e6e3e5f80>, 'EMA_fast': <data.processed.indicators_polars.EMA object at 0x7a7eb5752a10>, 'EMA_slow': <data.processed.indicators_polars.EMA object at 0x7a7e78060ed0>, 'RSI2': <data.processed.indicators_polars.RSI object at 0x7a7e78062250>, 'RSI6': <data.processed.indicators_polars.RSI object at 0x7a7e79fc2cd0>, 'realvol20': <function <lambda> at 0x7a7e6e3e5e40>, 'VWAP_dist': <function <lambda> at 0x7a7e6e3e5da0>, 'loc_in_range': <function <lambda> at 0x7a7e6e3e6200>, 'tod_sin': <function <lambda> at 0x7a7e6e3e6160>, 'tod_cos': <function <lambda> at 0x7a7e6e3e6020>, 'ema_slope': <function <lambda> at 0x7a7e6e3e60c0>, 'vol_slope': <function <lambda> at 0x7a7e6e3e62a0>, 'is_missing': <function <lambda> at 0x7a7e6e3e6340>}, statistics={'next_return': <function <lambda> at 0x7a7e6e3e63e0>, 'volatility': <function <lambda> at 0x7a7e6e3e6480>, 'spread': <function <lambda> at 0x7a7e6e3e6520>}, target=<data.processed.targets.TripleClassification object at 0x7a7e78148150>, normalizer=<data.processed.normalization.MinMaxNormalizerOverWindow object at 0x7a7e6eccec10>, missing_values_handler_polars=<data.processed.missing_values_handling.ContinuousForwardFillPolars object at 0x7a7e977eab90>, in_seq_len=60, horizon=1, multi_asset_prediction=True, validator=None), model_config=ModelConfig(model=TemporalSpatial(\n",
       "  (asset_embed): Embedding(50, 16)\n",
       "  (asset_proj): Linear(in_features=16, out_features=256, bias=False)\n",
       "  (lstm): LSTM(16, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (spatial_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "), registered_model_name='TemporalSpatial Regressor'), train_config=TrainConfig(loss_fn=MSELoss(), optimizer=AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: True\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 1e-10\n",
       "), scheduler={'type': 'OneCycleLR', 'max_lr': 0.003, 'pct_start': 0.1, 'div_factor': 25, 'final_div_factor': 1000.0, 'anneal_strategy': 'cos', 'cycle_momentum': False}, num_epochs=20, early_stopping_patience=10, device=device(type='cuda'), cudnn_benchmark=True, metrics={'rmse': <function rmse_regression at 0x7a7e6e3e4e00>}, batch_size=128, shuffle=True, num_workers=8, prefetch_factor=4, pin_memory=True, persistent_workers=True, drop_last=True, save_path=''), rl_config=RLConfig(trajectory_length=12, fee=0.0, spread_multiplier=0.67, trade_asset_count=1), observability_config=ObservabilityConfig(experiment_name='Return Regression MLP'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a3858fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 12:11:08,707 - INFO - Model compiled with torch.compile()\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=config.train_config.loss_fn,\n",
    "    optimizer=config.train_config.optimizer,\n",
    "    scheduler=config.train_config.scheduler,\n",
    "    num_epochs=config.train_config.num_epochs,\n",
    "    early_stopping_patience=config.train_config.early_stopping_patience,\n",
    "    device=config.train_config.device,\n",
    "    metrics=config.train_config.metrics,\n",
    "    save_path=config.train_config.save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f01c52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 12:11:08,749 - INFO - Epoch 1/20\n",
      "Training:   0%|          | 0/653 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "2026-01-05 12:11:29,223 - INFO - Train Loss: 0.2527        \n",
      "2026-01-05 12:11:29,224 - INFO - Train Rmse: 0.5012\n",
      "2026-01-05 12:11:29,224 - INFO - Val   Loss: 0.2355\n",
      "2026-01-05 12:11:29,224 - INFO - Val   Rmse: 0.4853\n",
      "2026-01-05 12:11:29,225 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:11:29,227 - INFO - \n",
      "2026-01-05 12:11:29,227 - INFO - Epoch 2/20\n",
      "2026-01-05 12:11:43,387 - INFO - Train Loss: 0.2376         \n",
      "2026-01-05 12:11:43,388 - INFO - Train Rmse: 0.4874\n",
      "2026-01-05 12:11:43,388 - INFO - Val   Loss: 0.2311\n",
      "2026-01-05 12:11:43,389 - INFO - Val   Rmse: 0.4807\n",
      "2026-01-05 12:11:43,389 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:11:43,391 - INFO - \n",
      "2026-01-05 12:11:43,391 - INFO - Epoch 3/20\n",
      "2026-01-05 12:11:57,536 - INFO - Train Loss: 0.2355         \n",
      "2026-01-05 12:11:57,536 - INFO - Train Rmse: 0.4852\n",
      "2026-01-05 12:11:57,537 - INFO - Val   Loss: 0.2313\n",
      "2026-01-05 12:11:57,537 - INFO - Val   Rmse: 0.4809\n",
      "2026-01-05 12:11:57,538 - INFO - \n",
      "2026-01-05 12:11:57,538 - INFO - Epoch 4/20\n",
      "2026-01-05 12:12:11,738 - INFO - Train Loss: 0.2346         \n",
      "2026-01-05 12:12:11,738 - INFO - Train Rmse: 0.4843\n",
      "2026-01-05 12:12:11,738 - INFO - Val   Loss: 0.2310\n",
      "2026-01-05 12:12:11,739 - INFO - Val   Rmse: 0.4806\n",
      "2026-01-05 12:12:11,739 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:12:11,741 - INFO - \n",
      "2026-01-05 12:12:11,742 - INFO - Epoch 5/20\n",
      "2026-01-05 12:12:27,774 - INFO - Train Loss: 0.2342         \n",
      "2026-01-05 12:12:27,774 - INFO - Train Rmse: 0.4839\n",
      "2026-01-05 12:12:27,775 - INFO - Val   Loss: 0.2306\n",
      "2026-01-05 12:12:27,775 - INFO - Val   Rmse: 0.4802\n",
      "2026-01-05 12:12:27,776 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:12:27,778 - INFO - \n",
      "2026-01-05 12:12:27,778 - INFO - Epoch 6/20\n",
      "2026-01-05 12:12:41,948 - INFO - Train Loss: 0.2338         \n",
      "2026-01-05 12:12:41,949 - INFO - Train Rmse: 0.4836\n",
      "2026-01-05 12:12:41,949 - INFO - Val   Loss: 0.2307\n",
      "2026-01-05 12:12:41,949 - INFO - Val   Rmse: 0.4803\n",
      "2026-01-05 12:12:41,950 - INFO - \n",
      "2026-01-05 12:12:41,950 - INFO - Epoch 7/20\n",
      "2026-01-05 12:12:56,719 - INFO - Train Loss: 0.2336         \n",
      "2026-01-05 12:12:56,719 - INFO - Train Rmse: 0.4833\n",
      "2026-01-05 12:12:56,720 - INFO - Val   Loss: 0.2304\n",
      "2026-01-05 12:12:56,720 - INFO - Val   Rmse: 0.4800\n",
      "2026-01-05 12:12:56,721 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:12:56,723 - INFO - \n",
      "2026-01-05 12:12:56,724 - INFO - Epoch 8/20\n",
      "2026-01-05 12:13:10,834 - INFO - Train Loss: 0.2335         \n",
      "2026-01-05 12:13:10,835 - INFO - Train Rmse: 0.4832\n",
      "2026-01-05 12:13:10,835 - INFO - Val   Loss: 0.2305\n",
      "2026-01-05 12:13:10,836 - INFO - Val   Rmse: 0.4801\n",
      "2026-01-05 12:13:10,836 - INFO - \n",
      "2026-01-05 12:13:10,836 - INFO - Epoch 9/20\n",
      "2026-01-05 12:13:25,194 - INFO - Train Loss: 0.2332         \n",
      "2026-01-05 12:13:25,195 - INFO - Train Rmse: 0.4829\n",
      "2026-01-05 12:13:25,195 - INFO - Val   Loss: 0.2300\n",
      "2026-01-05 12:13:25,195 - INFO - Val   Rmse: 0.4796\n",
      "2026-01-05 12:13:25,196 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:13:25,198 - INFO - \n",
      "2026-01-05 12:13:25,198 - INFO - Epoch 10/20\n",
      "2026-01-05 12:13:39,623 - INFO - Train Loss: 0.2329         \n",
      "2026-01-05 12:13:39,624 - INFO - Train Rmse: 0.4826\n",
      "2026-01-05 12:13:39,624 - INFO - Val   Loss: 0.2300\n",
      "2026-01-05 12:13:39,624 - INFO - Val   Rmse: 0.4795\n",
      "2026-01-05 12:13:39,625 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:13:39,627 - INFO - \n",
      "2026-01-05 12:13:39,627 - INFO - Epoch 11/20\n",
      "2026-01-05 12:13:54,157 - INFO - Train Loss: 0.2327         \n",
      "2026-01-05 12:13:54,158 - INFO - Train Rmse: 0.4824\n",
      "2026-01-05 12:13:54,158 - INFO - Val   Loss: 0.2299\n",
      "2026-01-05 12:13:54,158 - INFO - Val   Rmse: 0.4794\n",
      "2026-01-05 12:13:54,159 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:13:54,161 - INFO - \n",
      "2026-01-05 12:13:54,161 - INFO - Epoch 12/20\n",
      "2026-01-05 12:14:08,814 - INFO - Train Loss: 0.2325         \n",
      "2026-01-05 12:14:08,815 - INFO - Train Rmse: 0.4822\n",
      "2026-01-05 12:14:08,815 - INFO - Val   Loss: 0.2300\n",
      "2026-01-05 12:14:08,815 - INFO - Val   Rmse: 0.4796\n",
      "2026-01-05 12:14:08,816 - INFO - \n",
      "2026-01-05 12:14:08,816 - INFO - Epoch 13/20\n",
      "2026-01-05 12:14:23,511 - INFO - Train Loss: 0.2324         \n",
      "2026-01-05 12:14:23,511 - INFO - Train Rmse: 0.4821\n",
      "2026-01-05 12:14:23,512 - INFO - Val   Loss: 0.2298\n",
      "2026-01-05 12:14:23,513 - INFO - Val   Rmse: 0.4793\n",
      "2026-01-05 12:14:23,513 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:14:23,517 - INFO - \n",
      "2026-01-05 12:14:23,517 - INFO - Epoch 14/20\n",
      "2026-01-05 12:14:38,469 - INFO - Train Loss: 0.2324         \n",
      "2026-01-05 12:14:38,470 - INFO - Train Rmse: 0.4821\n",
      "2026-01-05 12:14:38,470 - INFO - Val   Loss: 0.2298\n",
      "2026-01-05 12:14:38,470 - INFO - Val   Rmse: 0.4794\n",
      "2026-01-05 12:14:38,471 - INFO - \n",
      "2026-01-05 12:14:38,471 - INFO - Epoch 15/20\n",
      "2026-01-05 12:14:52,895 - INFO - Train Loss: 0.2323         \n",
      "2026-01-05 12:14:52,895 - INFO - Train Rmse: 0.4820\n",
      "2026-01-05 12:14:52,896 - INFO - Val   Loss: 0.2298\n",
      "2026-01-05 12:14:52,896 - INFO - Val   Rmse: 0.4793\n",
      "2026-01-05 12:14:52,896 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:14:52,897 - INFO - \n",
      "2026-01-05 12:14:52,897 - INFO - Epoch 16/20\n",
      "2026-01-05 12:15:07,450 - INFO - Train Loss: 0.2322         \n",
      "2026-01-05 12:15:07,450 - INFO - Train Rmse: 0.4818\n",
      "2026-01-05 12:15:07,450 - INFO - Val   Loss: 0.2296\n",
      "2026-01-05 12:15:07,450 - INFO - Val   Rmse: 0.4792\n",
      "2026-01-05 12:15:07,451 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:15:07,452 - INFO - \n",
      "2026-01-05 12:15:07,452 - INFO - Epoch 17/20\n",
      "2026-01-05 12:15:22,010 - INFO - Train Loss: 0.2321         \n",
      "2026-01-05 12:15:22,010 - INFO - Train Rmse: 0.4818\n",
      "2026-01-05 12:15:22,011 - INFO - Val   Loss: 0.2296\n",
      "2026-01-05 12:15:22,011 - INFO - Val   Rmse: 0.4792\n",
      "2026-01-05 12:15:22,012 - INFO - \n",
      "2026-01-05 12:15:22,012 - INFO - Epoch 18/20\n",
      "2026-01-05 12:15:36,476 - INFO - Train Loss: 0.2321         \n",
      "2026-01-05 12:15:36,477 - INFO - Train Rmse: 0.4817\n",
      "2026-01-05 12:15:36,477 - INFO - Val   Loss: 0.2296\n",
      "2026-01-05 12:15:36,477 - INFO - Val   Rmse: 0.4791\n",
      "2026-01-05 12:15:36,478 - INFO - New best model found! Updating best state dict.\n",
      "2026-01-05 12:15:36,480 - INFO - \n",
      "2026-01-05 12:15:36,480 - INFO - Epoch 19/20\n",
      "2026-01-05 12:15:50,965 - INFO - Train Loss: 0.2321         \n",
      "2026-01-05 12:15:50,965 - INFO - Train Rmse: 0.4817\n",
      "2026-01-05 12:15:50,966 - INFO - Val   Loss: 0.2296\n",
      "2026-01-05 12:15:50,966 - INFO - Val   Rmse: 0.4792\n",
      "2026-01-05 12:15:50,966 - INFO - \n",
      "2026-01-05 12:15:50,967 - INFO - Epoch 20/20\n",
      "2026-01-05 12:16:05,353 - INFO - Train Loss: 0.2320         \n",
      "2026-01-05 12:16:05,354 - INFO - Train Rmse: 0.4817\n",
      "2026-01-05 12:16:05,354 - INFO - Val   Loss: 0.2296\n",
      "2026-01-05 12:16:05,355 - INFO - Val   Rmse: 0.4792\n",
      "2026-01-05 12:16:05,355 - INFO - \n"
     ]
    }
   ],
   "source": [
    "model, history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9955c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy  # Local import to avoid polluting global namespace unnecessarily\n",
    "state_dict = (\n",
    "    model.module.state_dict()\n",
    "        if isinstance(model, torch.nn.DataParallel)\n",
    "    else model.state_dict()\n",
    ")\n",
    "\n",
    "# Keep a local copy of the best weights so we can return the best model\n",
    "# after training finishes, without needing to reload from disk.\n",
    "best_model_state = copy.deepcopy(state_dict)\n",
    "\n",
    "# Persist to disk if a save_path was provided\n",
    "torch.save(state_dict, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed829636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 14:27:47,879 - INFO - Last timestamp counts across all slices and stocks:\n",
      "2026-01-05 14:27:47,880 - INFO -   2025-08-01 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,881 - INFO -   2025-08-04 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,882 - INFO -   2025-08-05 15:59:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,884 - INFO -   2025-08-05 16:00:00-04:00: 49 occurrences\n",
      "2026-01-05 14:27:47,885 - INFO -   2025-08-06 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,886 - INFO -   2025-08-07 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,887 - INFO -   2025-08-08 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,888 - INFO -   2025-08-11 15:59:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,889 - INFO -   2025-08-11 16:00:00-04:00: 49 occurrences\n",
      "2026-01-05 14:27:47,893 - INFO -   2025-08-12 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,894 - INFO -   2025-08-13 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,895 - INFO -   2025-08-14 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,896 - INFO -   2025-08-15 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,897 - INFO -   2025-08-18 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,899 - INFO -   2025-08-19 15:59:00-04:00: 2 occurrences\n",
      "2026-01-05 14:27:47,900 - INFO -   2025-08-19 16:00:00-04:00: 48 occurrences\n",
      "2026-01-05 14:27:47,901 - INFO -   2025-08-20 15:59:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,901 - INFO -   2025-08-20 16:00:00-04:00: 49 occurrences\n",
      "2026-01-05 14:27:47,903 - INFO -   2025-08-21 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,904 - INFO -   2025-08-22 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,905 - INFO -   2025-08-25 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,906 - INFO -   2025-08-26 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,907 - INFO -   2025-08-27 15:59:00-04:00: 2 occurrences\n",
      "2026-01-05 14:27:47,909 - INFO -   2025-08-27 16:00:00-04:00: 48 occurrences\n",
      "2026-01-05 14:27:47,910 - INFO -   2025-08-28 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,911 - INFO -   2025-08-29 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,914 - INFO -   2025-08-29 16:05:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,914 - INFO -   2025-08-29 16:50:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,916 - INFO -   2025-08-29 17:04:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,917 - INFO -   2025-08-29 17:29:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,918 - INFO -   2025-08-29 17:53:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,920 - INFO -   2025-08-29 18:32:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,921 - INFO -   2025-08-29 18:33:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,921 - INFO -   2025-08-29 18:48:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,922 - INFO -   2025-08-29 19:03:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,923 - INFO -   2025-08-29 19:09:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,924 - INFO -   2025-08-29 19:11:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,925 - INFO -   2025-08-29 19:24:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,926 - INFO -   2025-08-29 19:28:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,927 - INFO -   2025-08-29 19:31:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,928 - INFO -   2025-08-29 19:33:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,930 - INFO -   2025-08-29 19:37:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,931 - INFO -   2025-08-29 19:43:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,933 - INFO -   2025-08-29 19:47:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,934 - INFO -   2025-08-29 19:48:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,935 - INFO -   2025-08-29 19:53:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,937 - INFO -   2025-08-29 19:55:00-04:00: 2 occurrences\n",
      "2026-01-05 14:27:47,937 - INFO -   2025-08-29 19:57:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,939 - INFO -   2025-08-29 19:58:00-04:00: 4 occurrences\n",
      "2026-01-05 14:27:47,941 - INFO -   2025-08-29 19:59:00-04:00: 23 occurrences\n",
      "2026-01-05 14:27:47,941 - INFO -   2025-09-02 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,943 - INFO -   2025-09-03 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,945 - INFO -   2025-09-04 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,946 - INFO -   2025-09-05 15:59:00-04:00: 3 occurrences\n",
      "2026-01-05 14:27:47,948 - INFO -   2025-09-05 16:00:00-04:00: 47 occurrences\n",
      "2026-01-05 14:27:47,949 - INFO -   2025-09-08 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,951 - INFO -   2025-09-09 15:59:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,952 - INFO -   2025-09-09 16:00:00-04:00: 49 occurrences\n",
      "2026-01-05 14:27:47,953 - INFO -   2025-09-10 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,953 - INFO -   2025-09-11 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,955 - INFO -   2025-09-12 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,956 - INFO -   2025-09-15 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,958 - INFO -   2025-09-16 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,959 - INFO -   2025-09-17 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,961 - INFO -   2025-09-18 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,962 - INFO -   2025-09-19 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,963 - INFO -   2025-09-22 15:59:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,964 - INFO -   2025-09-22 16:00:00-04:00: 49 occurrences\n",
      "2026-01-05 14:27:47,965 - INFO -   2025-09-23 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,966 - INFO -   2025-09-24 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,967 - INFO -   2025-09-25 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,968 - INFO -   2025-09-26 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,969 - INFO -   2025-09-29 16:00:00-04:00: 50 occurrences\n",
      "2026-01-05 14:27:47,970 - INFO -   2025-09-30 15:59:00-04:00: 1 occurrences\n",
      "2026-01-05 14:27:47,973 - INFO -   2025-09-30 16:00:00-04:00: 49 occurrences\n"
     ]
    }
   ],
   "source": [
    "from config.experiments.cur_experiment import config\n",
    "from core_data_prep.core_data_prep import DataPreparer\n",
    "from core_inference.bars_response_handler import BarsResponseHandler\n",
    "from core_inference.quotes_response_handler import QuotesResponseHandler\n",
    "from core_inference.trader import Trader\n",
    "from core_inference.brokerage_proxies.alpaca_brokerage_proxy import AlpacaBrokerageProxy\n",
    "from core_inference.brokerage_proxies.backtest_brokerage_proxy import BacktestBrokerageProxy\n",
    "from core_inference.repository import Repository\n",
    "from core_inference.allocators.signal_predictor_allocator import SignalPredictorAllocator\n",
    "\n",
    "\n",
    "daily_slices = data_preparer._get_daily_slices(\n",
    "    retrieval_result,\n",
    "    start_date=config.data_config.val_set_last_date,\n",
    "    end_date=config.data_config.end,\n",
    "    slice_length=Constants.Data.TRADING_DAY_LENGTH_MINUTES + config.data_config.in_seq_len + config.data_config.normalizer.get_window() + 30,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "642dedb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 50, 541)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert all the slices have the same length\n",
    "assert len(set([len(cur_day_asset_slice) for cur_day_slices in daily_slices for cur_day_asset_slice in cur_day_slices.values()])) == 1\n",
    "\n",
    "len(daily_slices), len(daily_slices[0]), len(daily_slices[0]['AAPL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1fa580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikurnosau\\AppData\\Local\\Temp\\ipykernel_24160\\1264393750.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\n",
    "    \"../modeling/checkpoints/best_model.pth\",\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        new_state_dict[k.replace(\"_orig_mod.\", \"\")] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "allocator = SignalPredictorAllocator(\n",
    "    signal_predictor=model,\n",
    "    trade_asset_count=config.rl_config.trade_asset_count,\n",
    "    allow_short_positions=False\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0e2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 17:17:38,509 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:38,515 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:39,352 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:39,407 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:39,409 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:39,412 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:39,415 - INFO - Day 0 update 380 ended with cash 0.5000002086162567\n",
      "2026-01-06 17:17:39,535 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:39,540 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:40,304 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:40,346 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:40,347 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:40,351 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:40,354 - INFO - Day 0 update 381 ended with cash 0.49950759521105503\n",
      "2026-01-06 17:17:40,474 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:40,479 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:41,227 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:41,276 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:41,278 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:41,281 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:41,284 - INFO - Day 0 update 382 ended with cash 0.5000215884213017\n",
      "2026-01-06 17:17:41,415 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:41,420 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:42,169 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:42,237 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:42,239 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:42,243 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:42,246 - INFO - Day 0 update 383 ended with cash 0.49903715609696747\n",
      "2026-01-06 17:17:42,382 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:42,386 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:43,131 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:43,202 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:43,204 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:43,207 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:43,211 - INFO - Day 0 update 384 ended with cash 0.4990421468059682\n",
      "2026-01-06 17:17:43,341 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:43,347 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:44,090 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:44,133 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:44,134 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:44,137 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:44,141 - INFO - Day 0 update 385 ended with cash 0.49951146397770224\n",
      "2026-01-06 17:17:44,272 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:44,278 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:45,034 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:45,094 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:45,095 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:45,099 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:45,102 - INFO - Day 0 update 386 ended with cash 0.49929239536302045\n",
      "2026-01-06 17:17:45,229 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:45,237 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:45,963 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:45,989 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:45,991 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:45,996 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:45,999 - INFO - Day 0 update 387 ended with cash 0.4995218540922997\n",
      "2026-01-06 17:17:46,126 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:46,131 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:46,883 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:46,913 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:46,914 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:46,917 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:46,921 - INFO - Day 0 update 388 ended with cash 0.49837964762656645\n",
      "2026-01-06 17:17:47,048 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:47,054 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:48,411 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:48,476 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:48,477 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:48,480 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:48,483 - INFO - Day 0 update 389 ended with cash 0.49893910970082705\n",
      "2026-01-06 17:17:48,604 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:48,608 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:49,419 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:49,491 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:49,492 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:49,496 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:49,500 - INFO - Day 0 update 390 ended with cash 0.4976122291038617\n",
      "2026-01-06 17:17:49,503 - INFO - Day 0 ended with cash 0.9965187122961989\n",
      "2026-01-06 17:17:51,959 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:51,964 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:52,970 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:53,042 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:53,044 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:53,046 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:53,050 - INFO - Day 1 update 380 ended with cash 0.4982594749423872\n",
      "2026-01-06 17:17:53,172 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:53,178 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:54,155 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:54,186 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:54,187 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:54,190 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:54,195 - INFO - Day 1 update 381 ended with cash 0.49795291059662566\n",
      "2026-01-06 17:17:54,317 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:54,323 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:55,958 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:56,199 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:56,200 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:56,206 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:56,209 - INFO - Day 1 update 382 ended with cash 0.4984003874410958\n",
      "2026-01-06 17:17:56,381 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:56,387 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:57,461 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:57,489 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:57,490 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:57,525 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:57,530 - INFO - Day 1 update 383 ended with cash 0.4984529246062755\n",
      "2026-01-06 17:17:57,711 - INFO - Starting trading cycle...\n",
      "2026-01-06 17:17:57,717 - INFO - Transforming data for inference...\n",
      "2026-01-06 17:17:58,773 - INFO - Running portfolio allocator...\n",
      "2026-01-06 17:17:58,788 - INFO - Calculating position difference...\n",
      "2026-01-06 17:17:58,789 - INFO - Starting order execution...\n",
      "2026-01-06 17:17:58,793 - INFO - Order execution completed!\n",
      "2026-01-06 17:17:58,798 - INFO - Day 1 update 384 ended with cash 0.498275382393326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m     stock_data \u001b[38;5;241m=\u001b[39m stock_data_series\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     27\u001b[0m     stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stock_name\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mrepository\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m trader\u001b[38;5;241m.\u001b[39mperform_trading_cycle()\n\u001b[0;32m     32\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDay \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m update \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mupdate_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ended with cash \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbacktest_proxy\u001b[38;5;241m.\u001b[39mget_cash_balance()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\core_inference\\repository.py:41\u001b[0m, in \u001b[0;36mRepository.add_bar\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_bar\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m: Any]):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbars_and_quotes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msymbol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbars_and_quotes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msymbol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbid_price\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbid_price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbid_price\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbid_price[data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbid_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbid_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbid_size\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbid_size[data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_price\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask_price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask_price\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_price[data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask_size\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_size[data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     52\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\pandas\\core\\indexing.py:1932\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[0;32m   1931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m-> 1932\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\pandas\\core\\indexing.py:2328\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_maybe_update_cacher(clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\pandas\\core\\frame.py:10572\u001b[0m, in \u001b[0;36mDataFrame._append\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m  10569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m  10570\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m, other]\n\u001b[1;32m> 10572\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10576\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10577\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cur_cash = 1.\n",
    "for day_i, daily_slice in enumerate(daily_slices):\n",
    "    cur_day_initialization = {asset_name: asset_df.iloc[:-Constants.Data.TRADING_DAY_LENGTH_MINUTES].copy() for asset_name, asset_df in daily_slice.items()}\n",
    "    remaining_updates = [{asset_name: asset_df.iloc[-i] for asset_name, asset_df in daily_slice.items()} \\\n",
    "         for i in reversed(range(1, Constants.Data.TRADING_DAY_LENGTH_MINUTES + 1))]\n",
    "\n",
    "    repository = Repository(\n",
    "        trading_symbols=config.data_config.symbol_or_symbols,\n",
    "        required_history_depth=config.data_config.in_seq_len + config.data_config.normalizer.get_window() + 30,\n",
    "        bars_and_quotes=cur_day_initialization\n",
    "    )\n",
    "    backtest_proxy = BacktestBrokerageProxy(repository, config.rl_config.spread_multiplier, cur_cash)\n",
    "    trader = Trader(\n",
    "        data_preparer=data_preparer,\n",
    "        features=config.data_config.features_polars,\n",
    "        brokerage_proxy=backtest_proxy,\n",
    "        repository=repository,\n",
    "        portfolio_allocator=allocator\n",
    "    )\n",
    "\n",
    "    for update_i, remaining_update in enumerate(remaining_updates):\n",
    "        for stock_name, stock_data_series in remaining_update.items():\n",
    "            stock_data = stock_data_series.to_dict()\n",
    "            stock_data['symbol'] = stock_name\n",
    "            repository.add_bar(stock_data)\n",
    "\n",
    "        trader.perform_trading_cycle()\n",
    "\n",
    "        logging.info(f\"Day {day_i} update {update_i} ended with cash {backtest_proxy.get_cash_balance()}\")\n",
    "\n",
    "    backtest_proxy.close_all_positions()\n",
    "    cur_cash = backtest_proxy.get_cash_balance()\n",
    "    logging.info(f\"Day {day_i} ended with cash {cur_cash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc3121",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08ffa39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trader.states_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e11570aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(desired_position={'AAPL': 0.0, 'AMD': 0.0, 'BABA': 0.0, 'BITU': 0.0, 'C': 0.0, 'CSCO': 0.0, 'DAL': 0.0, 'DIA': 0.0, 'GLD': 0.0, 'GOOG': 0.0, 'IJR': 0.0, 'MARA': 0.0, 'MRVL': 0.0, 'MU': 0.0, 'NEE': 0.0, 'NKE': 0.0, 'NVDA': 0.0, 'ON': 0.0, 'PLTR': 0.0, 'PYPL': 0.0, 'QLD': 0.0, 'QQQ': 0.0, 'QQQM': 0.0, 'RKLB': 0.9999999, 'RSP': 0.0, 'SMCI': 0.0, 'SMH': 0.0, 'SOXL': 0.0, 'SOXX': 0.0, 'SPXL': 0.0, 'SPY': 0.0, 'TMF': 0.0, 'TNA': 0.0, 'TQQQ': 0.0, 'TSLA': 0.0, 'UBER': 0.0, 'UDOW': 0.0, 'UPRO': 0.0, 'VOO': 0.0, 'WFC': 0.0, 'XBI': 0.0, 'XLC': 0.0, 'XLE': 0.0, 'XLI': 0.0, 'XLK': 0.0, 'XLU': 0.0, 'XLV': 0.0, 'XLY': 0.0, 'XOM': 0.0, 'XRT': 0.0}, position={'AAPL': 0.0, 'AMD': 0.0, 'BABA': 0.0, 'BITU': 0.0, 'C': 0.0, 'CSCO': 0.0, 'DAL': 0.0, 'DIA': 0.0, 'GLD': 0.0, 'GOOG': 0.0, 'IJR': 0.0, 'MARA': 0.0, 'MRVL': 0.0, 'MU': 0.0, 'NEE': 0.0, 'NKE': 0.0, 'NVDA': 0.0, 'ON': 0.0, 'PLTR': 0.0, 'PYPL': 0.0, 'QLD': 0.0, 'QQQ': 0.0, 'QQQM': 0.0, 'RKLB': 1.0, 'RSP': 0.0, 'SMCI': 0.0, 'SMH': 0.0, 'SOXL': 0.0, 'SOXX': 0.0, 'SPXL': 0.0, 'SPY': 0.0, 'TMF': 0.0, 'TNA': 0.0, 'TQQQ': 0.0, 'TSLA': 0.0, 'UBER': 0.0, 'UDOW': 0.0, 'UPRO': 0.0, 'VOO': 0.0, 'WFC': 0.0, 'XBI': 0.0, 'XLC': 0.0, 'XLE': 0.0, 'XLI': 0.0, 'XLK': 0.0, 'XLU': 0.0, 'XLV': 0.0, 'XLY': 0.0, 'XOM': 0.0, 'XRT': 0.0}, available_cash=0.5250259300402593, shares_hold={'AAPL': 0.0, 'AMD': 0.0, 'BABA': 0.0, 'BITU': 0.0, 'C': 0.0, 'CSCO': 0.0, 'DAL': 0.0, 'DIA': 0.0, 'GLD': 0.0, 'GOOG': 0.0, 'IJR': 0.0, 'MARA': 0.0, 'MRVL': 0.0, 'MU': 0.0, 'NEE': 0.0, 'NKE': 0.0, 'NVDA': 0.0, 'ON': 0.0, 'PLTR': 0.0, 'PYPL': 0.0, 'QLD': 0.0, 'QQQ': 0.0, 'QQQM': 0.0, 'RKLB': 0.011880555346283125, 'RSP': 0.0, 'SMCI': 0.0, 'SMH': 0.0, 'SOXL': 0.0, 'SOXX': 0.0, 'SPXL': 0.0, 'SPY': 0.0, 'TMF': 0.0, 'TNA': 0.0, 'TQQQ': 0.0, 'TSLA': 0.0, 'UBER': 0.0, 'UDOW': 0.0, 'UPRO': 0.0, 'VOO': 0.0, 'WFC': 0.0, 'XBI': 0.0, 'XLC': 0.0, 'XLE': 0.0, 'XLI': 0.0, 'XLK': 0.0, 'XLU': 0.0, 'XLV': 0.0, 'XLY': 0.0, 'XOM': 0.0, 'XRT': 0.0}, _position_difference={'AAPL': 0.0, 'AMD': 0.0, 'BABA': 0.0, 'BITU': 0.0, 'C': 0.0, 'CSCO': 0.0, 'DAL': 0.0, 'DIA': 0.0, 'GLD': 0.0, 'GOOG': 0.0, 'IJR': 0.0, 'MARA': 0.0, 'MRVL': 0.0, 'MU': 0.0, 'NEE': 0.0, 'NKE': 0.0, 'NVDA': 0.0, 'ON': 0.0, 'PLTR': 0.0, 'PYPL': 0.0, 'QLD': 0.0, 'QQQ': 0.0, 'QQQM': 0.0, 'RKLB': 0.9999998807907104, 'RSP': 0.0, 'SMCI': 0.0, 'SMH': 0.0, 'SOXL': 0.0, 'SOXX': 0.0, 'SPXL': 0.0, 'SPY': -1.0, 'TMF': 0.0, 'TNA': 0.0, 'TQQQ': 0.0, 'TSLA': 0.0, 'UBER': 0.0, 'UDOW': 0.0, 'UPRO': 0.0, 'VOO': 0.0, 'WFC': 0.0, 'XBI': 0.0, 'XLC': 0.0, 'XLE': 0.0, 'XLI': 0.0, 'XLK': 0.0, 'XLU': 0.0, 'XLV': 0.0, 'XLY': 0.0, 'XOM': 0.0, 'XRT': 0.0}, _buy_positions={'RKLB': 0.9999998807907104}, _buy_cash_per_asset={'RKLB': 0.532248879513484}, _sell_positions={'SPY': 1.0}, _sell_percentage_per_share={'SPY': 1.0}, _sell_shares_per_asset={'SPY': 0.0008445056216047862})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trader.states_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa842d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
