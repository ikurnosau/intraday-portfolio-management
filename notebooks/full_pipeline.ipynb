{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a6dcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\models\\asu_classifier.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"Neighbourhood convolution using a (pre-)computed adjacency matrix.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime, timezone\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format for the log messages\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Log to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from data.raw.retrievers.alpaca_markets_retriever import AlpacaMarketsRetriever\n",
    "from config.constants import *\n",
    "from data.processed.dataset_creation import DatasetCreator\n",
    "from data.processed.indicators import *\n",
    "from data.processed.targets import Balanced3ClassClassification\n",
    "from data.processed.normalization import ZScoreOverWindowNormalizer, ZScoreNormalizer, MinMaxNormalizer\n",
    "from data.processed.dataset_pytorch import DatasetPytorch\n",
    "from modeling.trainer import Trainer\n",
    "from modeling.evaluate import evaluate_lgb_regressor, evaluate_torch_regressor, evaluate_torch_regressor_multiasset\n",
    "from observability.mlflow_integration import log_experiment\n",
    "\n",
    "from config.experiments.cur_experiment import config\n",
    "\n",
    "torch.backends.cudnn.benchmark = config.train_config.cudnn_benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755a729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = AlpacaMarketsRetriever(download_from_gdrive=False)\n",
    "\n",
    "retrieval_result = retriever.bars_with_quotes(\n",
    "    symbol_or_symbols=config.data_config.symbol_or_symbols, \n",
    "    start=config.data_config.start, \n",
    "    end=config.data_config.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d48d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 18:55:07,496 - INFO - Processing AAPL …\n",
      "2025-07-10 18:55:08,372 - INFO - Imputing 496 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:09,051 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:09,092 - INFO - Processing AMD …\n",
      "2025-07-10 18:55:09,752 - INFO - Imputing 214 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:10,413 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:10,456 - INFO - Processing BABA …\n",
      "2025-07-10 18:55:11,097 - INFO - Imputing 874 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:11,745 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:11,789 - INFO - Processing BITU …\n",
      "2025-07-10 18:55:12,423 - INFO - Imputing 6493 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:13,112 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:13,155 - INFO - Processing CSCO …\n",
      "2025-07-10 18:55:13,741 - INFO - Imputing 3929 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:14,409 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:14,458 - INFO - Processing C …\n",
      "2025-07-10 18:55:15,546 - INFO - Imputing 3733 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:16,155 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:16,192 - INFO - Processing DAL …\n",
      "2025-07-10 18:55:16,826 - INFO - Imputing 4112 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:17,605 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:17,650 - INFO - Processing DIA …\n",
      "2025-07-10 18:55:18,278 - INFO - Imputing 3842 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:18,988 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:19,025 - INFO - Processing GLD …\n",
      "2025-07-10 18:55:19,690 - INFO - Imputing 1989 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:20,327 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:20,365 - INFO - Processing GOOG …\n",
      "2025-07-10 18:55:20,943 - INFO - Imputing 1161 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:21,557 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:21,596 - INFO - Processing IJR …\n",
      "2025-07-10 18:55:22,336 - INFO - Imputing 5204 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:23,235 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:23,276 - INFO - Processing MARA …\n",
      "2025-07-10 18:55:24,236 - INFO - Imputing 108 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:25,147 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:25,190 - INFO - Processing MRVL …\n",
      "2025-07-10 18:55:25,991 - INFO - Imputing 2386 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:26,874 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:26,911 - INFO - Processing MU …\n",
      "2025-07-10 18:55:27,831 - INFO - Imputing 838 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:28,637 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:28,679 - INFO - Processing NEE …\n",
      "2025-07-10 18:55:29,456 - INFO - Imputing 4731 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:30,304 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:30,343 - INFO - Processing NKE …\n",
      "2025-07-10 18:55:31,487 - INFO - Imputing 2509 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:32,299 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:32,335 - INFO - Processing NVDA …\n",
      "2025-07-10 18:55:33,344 - INFO - Imputing 1 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:34,321 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:34,368 - INFO - Processing ON …\n",
      "2025-07-10 18:55:35,211 - INFO - Imputing 4325 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:36,073 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:36,115 - INFO - Processing PLTR …\n",
      "2025-07-10 18:55:37,057 - INFO - Imputing 58 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:37,910 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:37,945 - INFO - Processing PYPL …\n",
      "2025-07-10 18:55:38,765 - INFO - Imputing 3097 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:39,708 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:39,748 - INFO - Processing QLD …\n",
      "2025-07-10 18:55:40,633 - INFO - Imputing 4196 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:41,475 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:41,516 - INFO - Processing QQQM …\n",
      "2025-07-10 18:55:42,341 - INFO - Imputing 5090 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:43,300 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:43,347 - INFO - Processing QQQ …\n",
      "2025-07-10 18:55:44,329 - INFO - Imputing 152 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:45,160 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:45,196 - INFO - Processing RKLB …\n",
      "2025-07-10 18:55:46,101 - INFO - Imputing 659 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:46,933 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:46,971 - INFO - Processing RSP …\n",
      "2025-07-10 18:55:48,170 - INFO - Imputing 4643 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:48,987 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:49,021 - INFO - Processing SMCI …\n",
      "2025-07-10 18:55:49,975 - INFO - Imputing 242 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:50,810 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:50,846 - INFO - Processing SMH …\n",
      "2025-07-10 18:55:51,641 - INFO - Imputing 3394 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:52,508 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:52,551 - INFO - Processing SOXL …\n",
      "2025-07-10 18:55:53,746 - INFO - Imputing 17 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:54,688 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:54,732 - INFO - Processing SOXX …\n",
      "2025-07-10 18:55:55,615 - INFO - Imputing 4248 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:56,601 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:56,644 - INFO - Processing SPXL …\n",
      "2025-07-10 18:55:57,621 - INFO - Imputing 2257 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:58,359 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:58,395 - INFO - Processing SPY …\n",
      "2025-07-10 18:55:59,234 - INFO - Imputing 219 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:55:59,877 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:55:59,912 - INFO - Processing TMF …\n",
      "2025-07-10 18:56:00,592 - INFO - Imputing 539 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:01,306 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:01,346 - INFO - Processing TNA …\n",
      "2025-07-10 18:56:02,019 - INFO - Imputing 440 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:02,683 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:02,722 - INFO - Processing TQQQ …\n",
      "2025-07-10 18:56:03,417 - INFO - Imputing 37 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:04,067 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:04,104 - INFO - Processing TSLA …\n",
      "2025-07-10 18:56:04,780 - INFO - Imputing 2 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:05,422 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:05,456 - INFO - Processing UBER …\n",
      "2025-07-10 18:56:06,371 - INFO - Imputing 1667 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:07,009 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:07,049 - INFO - Processing UDOW …\n",
      "2025-07-10 18:56:07,693 - INFO - Imputing 5493 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:08,350 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:08,388 - INFO - Processing UPRO …\n",
      "2025-07-10 18:56:09,069 - INFO - Imputing 1797 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:09,724 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:09,764 - INFO - Processing VOO …\n",
      "2025-07-10 18:56:10,400 - INFO - Imputing 2312 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:11,031 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:11,065 - INFO - Processing WFC …\n",
      "2025-07-10 18:56:11,687 - INFO - Imputing 4302 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:12,322 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:12,357 - INFO - Processing XBI …\n",
      "2025-07-10 18:56:12,920 - INFO - Imputing 4076 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:13,571 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:13,614 - INFO - Processing XLC …\n",
      "2025-07-10 18:56:14,177 - INFO - Imputing 5351 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:14,789 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:14,822 - INFO - Processing XLE …\n",
      "2025-07-10 18:56:15,418 - INFO - Imputing 3826 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:16,067 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:16,102 - INFO - Processing XLI …\n",
      "2025-07-10 18:56:16,670 - INFO - Imputing 5077 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:17,297 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:17,332 - INFO - Processing XLK …\n",
      "2025-07-10 18:56:17,893 - INFO - Imputing 4014 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:18,531 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:18,568 - INFO - Processing XLU …\n",
      "2025-07-10 18:56:19,151 - INFO - Imputing 4835 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:19,782 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:19,812 - INFO - Processing XLV …\n",
      "2025-07-10 18:56:20,369 - INFO - Imputing 4922 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:21,310 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:21,343 - INFO - Processing XLY …\n",
      "2025-07-10 18:56:21,967 - INFO - Imputing 5146 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:22,621 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:22,650 - INFO - Processing XOM …\n",
      "2025-07-10 18:56:23,214 - INFO - Imputing 3570 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:23,805 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:23,834 - INFO - Processing XRT …\n",
      "2025-07-10 18:56:24,404 - INFO - Imputing 5599 NaN rows out of 97359 with forward fill..\n",
      "2025-07-10 18:56:25,007 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-07-10 18:56:25,039 - INFO - Finished feature generation. 0 assets skipped due to insufficient rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50, 79969, 60, 15),\n",
       " (50, 79969),\n",
       " (50, 79969),\n",
       " (50, 79969),\n",
       " (50, 7311, 60, 15),\n",
       " (50, 7311),\n",
       " (50, 7311),\n",
       " (50, 7311))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_creator = DatasetCreator(\n",
    "    features=config.data_config.features,\n",
    "    target=config.data_config.target,\n",
    "    normalizer=config.data_config.normalizer,\n",
    "    missing_values_handler=config.data_config.missing_values_handler,\n",
    "    train_set_last_date=config.data_config.train_set_last_date, \n",
    "    in_seq_len=config.data_config.in_seq_len,\n",
    "    multi_asset_prediction=config.data_config.multi_asset_prediction,\n",
    ")\n",
    "\n",
    "X_train, y_train, next_return_train, spread_train, X_test, y_test, next_return_test, spread_test = dataset_creator.create_dataset_numpy(retrieval_result)\n",
    "X_train.shape, y_train.shape, next_return_train.shape, spread_train.shape, X_test.shape, y_test.shape, next_return_test.shape, spread_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1596cfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.501904e-06"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_return_test.flatten().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f50717e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79969, 50, 60, 15),\n",
       " (79969, 50),\n",
       " (79969, 50),\n",
       " (79969, 50),\n",
       " (7311, 50, 60, 15),\n",
       " (7311, 50),\n",
       " (7311, 50),\n",
       " (7311, 50))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.data_config.multi_asset_prediction:\n",
    "    X_train = np.swapaxes(X_train, 0, 1)\n",
    "    y_train = np.swapaxes(y_train, 0, 1)\n",
    "    next_return_train = np.swapaxes(next_return_train, 0, 1)\n",
    "    spread_train = np.swapaxes(spread_train, 0, 1)\n",
    "\n",
    "    X_test = np.swapaxes(X_test, 0, 1)\n",
    "    y_test = np.swapaxes(y_test, 0, 1)\n",
    "    next_return_test = np.swapaxes(next_return_test, 0, 1)\n",
    "    spread_test = np.swapaxes(spread_test, 0, 1)\n",
    "\n",
    "X_train.shape, y_train.shape, next_return_train.shape, spread_train.shape, X_test.shape, y_test.shape, next_return_test.shape, spread_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc91696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49992302, 0.501705)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98a4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetPytorch(X_train, y_train, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")\n",
    "test_loader = DatasetPytorch(X_test, y_test, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4b5aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASUClassifier(\n",
       "  (sagcn): _SAGCN(\n",
       "    (start_conv): Conv2d(15, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn_start): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (tcns): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 2), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 2), stride=(1, 1), dilation=(1, 2))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 2), stride=(1, 1), dilation=(1, 4))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 2), stride=(1, 1), dilation=(1, 8))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (gcns): ModuleList(\n",
       "      (0-3): 4 x _GraphConvNet(\n",
       "        (_nconv): _NConv()\n",
       "        (mlp): _Linear1x1(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (sans): ModuleList(\n",
       "      (0): _SpatialAttention(\n",
       "        (W1): Linear(in_features=59, out_features=1, bias=False)\n",
       "        (W2): Linear(in_features=128, out_features=59, bias=False)\n",
       "        (W3): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (bn_w1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_w3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_w2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): _SpatialAttention(\n",
       "        (W1): Linear(in_features=58, out_features=1, bias=False)\n",
       "        (W2): Linear(in_features=128, out_features=58, bias=False)\n",
       "        (W3): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (bn_w1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_w3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_w2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): _SpatialAttention(\n",
       "        (W1): Linear(in_features=56, out_features=1, bias=False)\n",
       "        (W2): Linear(in_features=128, out_features=56, bias=False)\n",
       "        (W3): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (bn_w1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_w3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_w2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): _SpatialAttention(\n",
       "        (W1): Linear(in_features=52, out_features=1, bias=False)\n",
       "        (W2): Linear(in_features=128, out_features=52, bias=False)\n",
       "        (W3): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (bn_w1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_w3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_w2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (residual_convs): ModuleList(\n",
       "      (0-3): 4 x Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (bns): ModuleList(\n",
       "      (0-3): 4 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = config.model_config.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3858fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    loss_fn=config.train_config.loss_fn,\n",
    "    optimizer=config.train_config.optimizer,\n",
    "    scheduler=config.train_config.scheduler,\n",
    "    num_epochs=config.train_config.num_epochs,\n",
    "    device=config.train_config.device,\n",
    "    metrics=config.train_config.metrics,\n",
    "    save_path=config.train_config.save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01c52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 18:56:39,687 - INFO - Epoch 1/1\n",
      "                                                 \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't open shared file mapping: <torch_37280_3582731690_0>, error code: <1450>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\trainer.py:75\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     73\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m     train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     val_loss, val_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\trainer.py:126\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    124\u001b[0m total_metrics \u001b[38;5;241m=\u001b[39m {name: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m--> 126\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:479\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistent_workers \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 479\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\u001b[38;5;241m.\u001b[39m_reset(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py:607\u001b[0m, in \u001b[0;36mreduce_storage\u001b[1;34m(storage)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pickle meta storage; try pickling a meta tensor instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_sharing_strategy() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_system\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 607\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    609\u001b[0m     rebuild \u001b[38;5;241m=\u001b[39m rebuild_storage_filename\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\storage.py:437\u001b[0m, in \u001b[0;36m_share_memory_lock_protected.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;66;03m# If we acquired the storage lock here and we're done working on it\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;66;03m# we can now release it and free the entry.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m to_free \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;66;03m# Ensure that the cdata from the storage didn't change and only\u001b[39;00m\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;66;03m# the data_ptr did.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\storage.py:516\u001b[0m, in \u001b[0;36mUntypedStorage._share_filename_cpu_\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;129m@_share_memory_lock_protected\u001b[39m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_share_filename_cpu_\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't open shared file mapping: <torch_37280_3582731690_0>, error code: <1450>"
     ]
    }
   ],
   "source": [
    "model, history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3802d5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.62 GiB. GPU 0 has a total capacity of 8.00 GiB of which 1.71 GiB is free. Of the allocated memory 2.64 GiB is allocated by PyTorch, and 2.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate_torch_regressor_multiasset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_return_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspread_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\evaluate.py:146\u001b[0m, in \u001b[0;36mevaluate_torch_regressor_multiasset\u001b[1;34m(model, X_train, y_train, X_test, y_test, test_return, test_spread, trade_asset_count)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Run batched predictions\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m train_preds_arr \u001b[38;5;241m=\u001b[39m \u001b[43m_predict_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m test_preds_arr \u001b[38;5;241m=\u001b[39m _predict_in_batches(X_test)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Select the *trade_asset_count* most confident assets per time step\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\evaluate.py:136\u001b[0m, in \u001b[0;36mevaluate_torch_regressor_multiasset.<locals>._predict_in_batches\u001b[1;34m(X, batch_size)\u001b[0m\n\u001b[0;32m    134\u001b[0m batch \u001b[38;5;241m=\u001b[39m X[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m    135\u001b[0m batch_t \u001b[38;5;241m=\u001b[39m _to_tensor(batch, device, torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 136\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# squeeze regression singleton dim if present\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\models\\tsa_classifier.py:78\u001b[0m, in \u001b[0;36mTemporalSpatial.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# (B*A, T, F) → LSTM → (B*A, H)\u001b[39;00m\n\u001b[0;32m     77\u001b[0m x_flat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m A, T, F)\n\u001b[1;32m---> 78\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m h \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# last time step (B*A, H)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(h)\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.62 GiB. GPU 0 has a total capacity of 8.00 GiB of which 1.71 GiB is free. Of the allocated memory 2.64 GiB is allocated by PyTorch, and 2.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "evaluate_torch_regressor_multiasset(model, X_train, y_train, X_test, y_test, next_return_test, spread_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed829636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/26 15:35:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'LSTM Default' already exists. Creating a new version of this model...\n",
      "2025/06/26 15:35:14 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LSTM Default, version 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run gentle-loon-699 at: http://127.0.0.1:8080/#/experiments/439216085822475480/runs/54deb1104660468d9ffb4e7e278e9cfb\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/439216085822475480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '10' of model 'LSTM Default'.\n"
     ]
    }
   ],
   "source": [
    "log_experiment(\n",
    "    config=config, \n",
    "    model=model, \n",
    "    history=history,\n",
    "    input_data_sample=next(iter(train_loader))[0].to(trainer.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c286205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9435\n",
      "[LightGBM] [Info] Number of data points in the train set: 7371, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.497863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train rmse: 0.26411260601695974, Test rmse: 0.2684210886033184, Baseline rmse: 0.2599985897541046\n",
      "Expected return: 0.00010183148393891163, Baseline return: 2.569958041931386e-06, Max possible return 0.00048079571570269763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluate_lgb_regressor(X_train, y_train, X_test, y_test, next_return_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
