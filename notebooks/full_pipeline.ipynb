{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a6dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime, timezone\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format for the log messages\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Log to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from data.raw.retrievers.alpaca_markets_retriever import AlpacaMarketsRetriever\n",
    "from config.constants import *\n",
    "from data.processed.dataset_creation import DatasetCreator\n",
    "from data.processed.indicators import *\n",
    "from data.processed.targets import Balanced3ClassClassification\n",
    "from data.processed.normalization import ZScoreOverWindowNormalizer, ZScoreNormalizer, MinMaxNormalizer\n",
    "from data.processed.dataset_pytorch import DatasetPytorch\n",
    "from modeling.trainer import Trainer\n",
    "from modeling.evaluate import evaluate_lgb_regressor, evaluate_torch_regressor, evaluate_torch_regressor_multiasset\n",
    "# from observability.mlflow_integration import log_experiment\n",
    "\n",
    "from modeling.rl.environment import PortfolioEnvironment\n",
    "from modeling.rl.state import State\n",
    "from modeling.rl.agent import RlAgent\n",
    "from modeling.rl.algorithms.policy_gradient import PolicyGradient\n",
    "from modeling.rl.actors.actor import RlActor, FullyConnectedBackend, TransformerBackend\n",
    "from modeling.rl.actors.signal_predictor_actor import SignalPredictorActor\n",
    "from modeling.rl.actors.high_energy_low_friction_actor import HighEnergyLowFrictionActor\n",
    "from modeling.rl.actors.xsmom_actor import XSMomActor\n",
    "from modeling.rl.actors.tsmom_actor import TSMomActor\n",
    "from modeling.rl.actors.blsw_actor import BLSWActor\n",
    "from modeling.rl.trajectory_dataset import TrajectoryDataset\n",
    "from modeling.rl.metrics import MetricsCalculator, DEFAULT_METRICS\n",
    "from modeling.rl.reward import EstimatedReturnReward\n",
    "from modeling.rl.loss import SumLogReturnLoss\n",
    "from modeling.rl.visualization.wealth_plot import plot_cumulative_wealth\n",
    "from modeling.rl.visualization.position_plot import plot_position_heatmap\n",
    "from config.experiments.cur_experiment import config\n",
    "\n",
    "torch.backends.cudnn.benchmark = config.train_config.cudnn_benchmark\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63c4606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1Min'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(config.data_config.frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "755a729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = AlpacaMarketsRetriever(download_from_gdrive=False, timeframe=config.data_config.frequency)\n",
    "\n",
    "retrieval_result = retriever.bars_with_quotes(\n",
    "    symbol_or_symbols=config.data_config.symbol_or_symbols, \n",
    "    start=config.data_config.start, \n",
    "    end=config.data_config.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d48d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 19:13:40,277 - INFO - Processing AAPL …\n",
      "2025-10-16 19:13:40,594 - INFO - Imputing 496 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:40,847 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:40,915 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:40,937 - INFO - Processing AMD …\n",
      "2025-10-16 19:13:41,375 - INFO - Imputing 214 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:41,615 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:41,679 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:41,699 - INFO - Processing BABA …\n",
      "2025-10-16 19:13:42,020 - INFO - Imputing 874 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:42,271 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:42,337 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:42,357 - INFO - Processing BITU …\n",
      "2025-10-16 19:13:42,678 - INFO - Imputing 6493 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:42,955 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:43,023 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:43,045 - INFO - Processing CSCO …\n",
      "2025-10-16 19:13:43,369 - INFO - Imputing 3929 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:43,635 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:43,702 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:43,726 - INFO - Processing C …\n",
      "2025-10-16 19:13:44,032 - INFO - Imputing 3733 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:44,490 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:44,558 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:44,583 - INFO - Processing DAL …\n",
      "2025-10-16 19:13:44,901 - INFO - Imputing 4112 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:45,201 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:45,270 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:45,294 - INFO - Processing DIA …\n",
      "2025-10-16 19:13:45,681 - INFO - Imputing 3842 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:45,989 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:46,088 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:46,117 - INFO - Processing GLD …\n",
      "2025-10-16 19:13:46,567 - INFO - Imputing 1989 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:46,967 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:47,056 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:47,077 - INFO - Processing GOOG …\n",
      "2025-10-16 19:13:47,485 - INFO - Imputing 1161 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:47,830 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:47,928 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:47,952 - INFO - Processing IJR …\n",
      "2025-10-16 19:13:48,382 - INFO - Imputing 5204 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:48,732 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:48,823 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:48,850 - INFO - Processing MARA …\n",
      "2025-10-16 19:13:49,333 - INFO - Imputing 108 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:49,676 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:49,773 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:49,795 - INFO - Processing MRVL …\n",
      "2025-10-16 19:13:50,461 - INFO - Imputing 2386 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:50,824 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:50,921 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:50,949 - INFO - Processing MU …\n",
      "2025-10-16 19:13:51,401 - INFO - Imputing 838 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:51,764 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:51,858 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:51,884 - INFO - Processing NEE …\n",
      "2025-10-16 19:13:52,300 - INFO - Imputing 4731 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:52,659 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:52,749 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:52,776 - INFO - Processing NKE …\n",
      "2025-10-16 19:13:53,214 - INFO - Imputing 2509 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:53,559 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:53,651 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:53,678 - INFO - Processing NVDA …\n",
      "2025-10-16 19:13:54,194 - INFO - Imputing 1 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:54,561 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:54,660 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:54,690 - INFO - Processing ON …\n",
      "2025-10-16 19:13:55,119 - INFO - Imputing 4325 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:55,438 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:55,528 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:55,552 - INFO - Processing PLTR …\n",
      "2025-10-16 19:13:55,994 - INFO - Imputing 58 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:56,322 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:56,413 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:56,434 - INFO - Processing PYPL …\n",
      "2025-10-16 19:13:56,824 - INFO - Imputing 3097 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:57,157 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:57,247 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:57,268 - INFO - Processing QLD …\n",
      "2025-10-16 19:13:57,672 - INFO - Imputing 4196 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:57,988 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:58,076 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:58,095 - INFO - Processing QQQM …\n",
      "2025-10-16 19:13:58,465 - INFO - Imputing 5090 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:58,770 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:58,859 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:58,877 - INFO - Processing QQQ …\n",
      "2025-10-16 19:13:59,520 - INFO - Imputing 152 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:13:59,830 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:13:59,924 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:13:59,950 - INFO - Processing RKLB …\n",
      "2025-10-16 19:14:00,366 - INFO - Imputing 659 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:00,668 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:00,763 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:00,794 - INFO - Processing RSP …\n",
      "2025-10-16 19:14:01,205 - INFO - Imputing 4643 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:01,532 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:01,621 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:01,646 - INFO - Processing SMCI …\n",
      "2025-10-16 19:14:02,062 - INFO - Imputing 242 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:02,380 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:02,477 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:02,500 - INFO - Processing SMH …\n",
      "2025-10-16 19:14:02,885 - INFO - Imputing 3394 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:03,220 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:03,327 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:03,349 - INFO - Processing SOXL …\n",
      "2025-10-16 19:14:03,846 - INFO - Imputing 17 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:04,167 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:04,258 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:04,280 - INFO - Processing SOXX …\n",
      "2025-10-16 19:14:04,657 - INFO - Imputing 4248 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:04,971 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:05,060 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:05,083 - INFO - Processing SPXL …\n",
      "2025-10-16 19:14:05,484 - INFO - Imputing 2257 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:05,846 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:05,949 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:05,975 - INFO - Processing SPY …\n",
      "2025-10-16 19:14:06,453 - INFO - Imputing 219 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:06,795 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:06,890 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:06,919 - INFO - Processing TMF …\n",
      "2025-10-16 19:14:07,640 - INFO - Imputing 539 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:08,004 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:08,107 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:08,134 - INFO - Processing TNA …\n",
      "2025-10-16 19:14:08,567 - INFO - Imputing 440 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:08,874 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:08,965 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:08,986 - INFO - Processing TQQQ …\n",
      "2025-10-16 19:14:09,437 - INFO - Imputing 37 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:09,741 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:09,829 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:09,852 - INFO - Processing TSLA …\n",
      "2025-10-16 19:14:10,331 - INFO - Imputing 2 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:10,639 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:10,731 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:10,752 - INFO - Processing UBER …\n",
      "2025-10-16 19:14:11,132 - INFO - Imputing 1667 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:11,444 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:11,538 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:11,561 - INFO - Processing UDOW …\n",
      "2025-10-16 19:14:11,954 - INFO - Imputing 5493 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:12,290 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:12,384 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:12,406 - INFO - Processing UPRO …\n",
      "2025-10-16 19:14:12,839 - INFO - Imputing 1797 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:13,155 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:13,245 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:13,268 - INFO - Processing VOO …\n",
      "2025-10-16 19:14:13,653 - INFO - Imputing 2312 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:13,962 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:14,060 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:14,083 - INFO - Processing WFC …\n",
      "2025-10-16 19:14:14,484 - INFO - Imputing 4302 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:14,873 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:14,972 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:14,999 - INFO - Processing XBI …\n",
      "2025-10-16 19:14:15,853 - INFO - Imputing 4076 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:16,261 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:16,370 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:16,405 - INFO - Processing XLC …\n",
      "2025-10-16 19:14:16,968 - INFO - Imputing 5351 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:17,425 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:17,557 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:17,594 - INFO - Processing XLE …\n",
      "2025-10-16 19:14:18,075 - INFO - Imputing 3826 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:18,473 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:18,610 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:18,665 - INFO - Processing XLI …\n",
      "2025-10-16 19:14:19,227 - INFO - Imputing 5077 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:19,590 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:19,680 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:19,700 - INFO - Processing XLK …\n",
      "2025-10-16 19:14:20,181 - INFO - Imputing 4014 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:20,593 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:20,698 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:20,724 - INFO - Processing XLU …\n",
      "2025-10-16 19:14:21,179 - INFO - Imputing 4835 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:21,528 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:21,621 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:21,645 - INFO - Processing XLV …\n",
      "2025-10-16 19:14:21,999 - INFO - Imputing 4922 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:22,316 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:22,404 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:22,423 - INFO - Processing XLY …\n",
      "2025-10-16 19:14:22,773 - INFO - Imputing 5146 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:23,101 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:23,195 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:23,218 - INFO - Processing XOM …\n",
      "2025-10-16 19:14:23,623 - INFO - Imputing 3570 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:23,960 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:24,055 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:14:24,080 - INFO - Processing XRT …\n",
      "2025-10-16 19:14:24,440 - INFO - Imputing 5599 NaN rows out of 97359 with forward fill..\n",
      "2025-10-16 19:14:24,807 - INFO - Spread has 0 NaNs\n",
      "2025-10-16 19:14:24,895 - INFO - Imputing 39 NaN rows with 0.5 sentinel value\n",
      "2025-10-16 19:15:53,979 - INFO - Finished feature generation. Dropped 0 assets by length threshold. Kept 50 assets with 87150 aligned rows each. Max features len prior to alignment: 87150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((72391, 50, 60, 15),\n",
       " (72391, 50),\n",
       " (72391, 50),\n",
       " (72391, 50),\n",
       " (72391, 50),\n",
       " (7350, 50, 60, 15),\n",
       " (7350, 50),\n",
       " (7350, 50),\n",
       " (7350, 50),\n",
       " (7350, 50),\n",
       " (7350, 50, 60, 15),\n",
       " (7350, 50),\n",
       " (7350, 50),\n",
       " (7350, 50),\n",
       " (7350, 50))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_creator = DatasetCreator(\n",
    "    features=config.data_config.features,\n",
    "    target=config.data_config.target,\n",
    "    normalizer=config.data_config.normalizer,\n",
    "    missing_values_handler=config.data_config.missing_values_handler,\n",
    "    train_set_last_date=config.data_config.train_set_last_date, \n",
    "    val_set_last_date=config.data_config.val_set_last_date,\n",
    "    cutoff_time=config.data_config.cutoff_time,\n",
    "    in_seq_len=config.data_config.in_seq_len,\n",
    "    multi_asset_prediction=config.data_config.multi_asset_prediction,\n",
    ")\n",
    "\n",
    "X_train, y_train, next_return_train, spread_train, volatility_train, \\\n",
    "    X_val, y_val, next_return_val, spread_val, volatility_val, \\\n",
    "    X_test, y_test, next_return_test, spread_test, volatility_test \\\n",
    "        = dataset_creator.create_dataset_numpy(retrieval_result)\n",
    "\n",
    "X_train.shape, y_train.shape, next_return_train.shape, spread_train.shape, volatility_train.shape, \\\n",
    "    X_val.shape, y_val.shape, next_return_val.shape, spread_val.shape, volatility_val.shape, \\\n",
    "        X_test.shape, y_test.shape, next_return_test.shape, spread_test.shape, volatility_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc91696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4998529, 0.4990973, 0.5015)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean(), y_val.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85079cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \t    next_return, spread, volatility\n",
    "# train (0.00062720885, 0.00025727754, 0.000789116)\n",
    "# val   (0.0011397429, 0.0002572774, 0.0014156421)\n",
    "# test  (0.0005497588, 0.0002572774, 0.00068560627)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a11c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(next_return_train).mean(), spread_train.mean(), volatility_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d798023",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(next_return_val).mean(), spread_val.mean(), volatility_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677eea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(next_return_test).mean(), spread_test.mean(), volatility_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetPytorch(X_train, y_train, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")\n",
    "val_loader = DatasetPytorch(X_val, y_val, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4b5aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalSpatial(\n",
       "  (asset_embed): Embedding(50, 16)\n",
       "  (asset_proj): Linear(in_features=16, out_features=256, bias=False)\n",
       "  (lstm): LSTM(15, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (spatial_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = config.model_config.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa26ca6-8128-4e85-be01-34335ab51675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentConfig(data_config=DataConfig(symbol_or_symbols=['AAPL', 'AMD', 'BABA', 'BITU', 'C', 'CSCO', 'DAL', 'DIA', 'GLD', 'GOOG', 'IJR', 'MARA', 'MRVL', 'MU', 'NEE', 'NKE', 'NVDA', 'ON', 'PLTR', 'PYPL', 'QLD', 'QQQ', 'QQQM', 'RKLB', 'RSP', 'SMCI', 'SMH', 'SOXL', 'SOXX', 'SPXL', 'SPY', 'TMF', 'TNA', 'TQQQ', 'TSLA', 'UBER', 'UDOW', 'UPRO', 'VOO', 'WFC', 'XBI', 'XLC', 'XLE', 'XLI', 'XLK', 'XLU', 'XLV', 'XLY', 'XOM', 'XRT'], start=datetime.datetime(2024, 6, 1, 0, 0), end=datetime.datetime(2025, 6, 1, 0, 0), features={'log_ret': <function <lambda> at 0x7f96b70f6ca0>, 'hl_range': <function <lambda> at 0x7f96b70f6de0>, 'close_open': <function <lambda> at 0x7f96b6f08a40>, 'vol_delta': <function <lambda> at 0x7f96b6f08ae0>, 'EMA_fast': <data.processed.indicators.EMA object at 0x7f96b70bda90>, 'EMA_slow': <data.processed.indicators.EMA object at 0x7f96b709cb90>, 'RSI2': <data.processed.indicators.RSI object at 0x7f96b704ff50>, 'RSI6': <data.processed.indicators.RSI object at 0x7f96c720af90>, 'realvol20': <function <lambda> at 0x7f96b6f08b80>, 'VWAP_dist': <function <lambda> at 0x7f96b6f08c20>, 'loc_in_range': <function <lambda> at 0x7f96b6f08cc0>, 'tod_sin': <function <lambda> at 0x7f96b6f08d60>, 'tod_cos': <function <lambda> at 0x7f96b6f08e00>, 'ema_slope': <function <lambda> at 0x7f96b6f08ea0>, 'vol_slope': <function <lambda> at 0x7f96b6f08f40>}, target=<data.processed.targets.FutureMeanReturnClassification object at 0x7f96b70f2d50>, normalizer=<data.processed.normalization.MinMaxNormalizerOverWindow object at 0x7f96b70d5450>, missing_values_handler=<data.processed.missing_values_handling.ForwardFillFlatBars object at 0x7f96be58cb10>, in_seq_len=60, train_set_last_date=datetime.datetime(2025, 5, 1, 0, 0, tzinfo=datetime.timezone.utc), multi_asset_prediction=True, cutoff_time=datetime.time(14, 10)), model_config=ModelConfig(model=TemporalSpatial(\n",
       "  (asset_embed): Embedding(50, 16)\n",
       "  (asset_proj): Linear(in_features=16, out_features=256, bias=False)\n",
       "  (lstm): LSTM(15, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (spatial_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "), registered_model_name='TemporalSpatial Regressor'), train_config=TrainConfig(loss_fn=MSELoss(), optimizer=AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: True\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 1e-10\n",
       "), scheduler={'type': 'OneCycleLR', 'max_lr': 0.003, 'pct_start': 0.1, 'div_factor': 25, 'final_div_factor': 1000.0, 'anneal_strategy': 'cos', 'cycle_momentum': False}, num_epochs=20, early_stopping_patience=5, device=device(type='cuda'), cudnn_benchmark=True, metrics={'rmse': <function rmse_regression at 0x7f96b6f080e0>}, batch_size=128, shuffle=True, num_workers=8, prefetch_factor=4, pin_memory=True, persistent_workers=True, drop_last=True, save_path=''), observability_config=ObservabilityConfig(experiment_name='Return Regression MLP'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3858fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=config.train_config.loss_fn,\n",
    "    optimizer=config.train_config.optimizer,\n",
    "    scheduler=config.train_config.scheduler,\n",
    "    num_epochs=config.train_config.num_epochs,\n",
    "    early_stopping_patience=config.train_config.early_stopping_patience,\n",
    "    device=config.train_config.device,\n",
    "    metrics=config.train_config.metrics,\n",
    "    save_path=config.train_config.save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6d315-3bf8-452a-ba17-6badd562dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1154\n",
    "# 0.3397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01c52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 16:14:37,973 - INFO - Epoch 1/20\n",
      "2025-08-11 16:15:24,477 - INFO - Train Loss: 0.1358        \n",
      "2025-08-11 16:15:24,477 - INFO - Train Rmse: 0.3670\n",
      "2025-08-11 16:15:24,477 - INFO - Val   Loss: 0.1219\n",
      "2025-08-11 16:15:24,477 - INFO - Val   Rmse: 0.3490\n",
      "2025-08-11 16:15:24,477 - INFO - \n",
      "2025-08-11 16:15:24,479 - INFO - Epoch 2/20\n",
      "2025-08-11 16:16:05,813 - INFO - Train Loss: 0.1253        \n",
      "2025-08-11 16:16:05,814 - INFO - Train Rmse: 0.3539\n",
      "2025-08-11 16:16:05,814 - INFO - Val   Loss: 0.1171\n",
      "2025-08-11 16:16:05,814 - INFO - Val   Rmse: 0.3421\n",
      "2025-08-11 16:16:05,815 - INFO - \n",
      "2025-08-11 16:16:05,816 - INFO - Epoch 3/20\n",
      "2025-08-11 16:16:47,121 - INFO - Train Loss: 0.1240        \n",
      "2025-08-11 16:16:47,121 - INFO - Train Rmse: 0.3521\n",
      "2025-08-11 16:16:47,122 - INFO - Val   Loss: 0.1160\n",
      "2025-08-11 16:16:47,122 - INFO - Val   Rmse: 0.3405\n",
      "2025-08-11 16:16:47,122 - INFO - \n",
      "2025-08-11 16:16:47,123 - INFO - Epoch 4/20\n",
      "2025-08-11 16:17:28,418 - INFO - Train Loss: 0.1235        \n",
      "2025-08-11 16:17:28,419 - INFO - Train Rmse: 0.3514\n",
      "2025-08-11 16:17:28,419 - INFO - Val   Loss: 0.1161\n",
      "2025-08-11 16:17:28,419 - INFO - Val   Rmse: 0.3408\n",
      "2025-08-11 16:17:28,419 - INFO - \n",
      "2025-08-11 16:17:28,419 - INFO - Epoch 5/20\n",
      "2025-08-11 16:18:09,781 - INFO - Train Loss: 0.1231        \n",
      "2025-08-11 16:18:09,782 - INFO - Train Rmse: 0.3507\n",
      "2025-08-11 16:18:09,782 - INFO - Val   Loss: 0.1160\n",
      "2025-08-11 16:18:09,782 - INFO - Val   Rmse: 0.3406\n",
      "2025-08-11 16:18:09,782 - INFO - \n",
      "2025-08-11 16:18:09,782 - INFO - Epoch 6/20\n",
      "2025-08-11 16:18:51,146 - INFO - Train Loss: 0.1228        \n",
      "2025-08-11 16:18:51,146 - INFO - Train Rmse: 0.3504\n",
      "2025-08-11 16:18:51,146 - INFO - Val   Loss: 0.1157\n",
      "2025-08-11 16:18:51,146 - INFO - Val   Rmse: 0.3402\n",
      "2025-08-11 16:18:51,146 - INFO - \n",
      "2025-08-11 16:18:51,148 - INFO - Epoch 7/20\n",
      "2025-08-11 16:19:32,458 - INFO - Train Loss: 0.1226        \n",
      "2025-08-11 16:19:32,459 - INFO - Train Rmse: 0.3501\n",
      "2025-08-11 16:19:32,459 - INFO - Val   Loss: 0.1158\n",
      "2025-08-11 16:19:32,459 - INFO - Val   Rmse: 0.3402\n",
      "2025-08-11 16:19:32,459 - INFO - \n",
      "2025-08-11 16:19:32,459 - INFO - Epoch 8/20\n",
      "2025-08-11 16:20:13,769 - INFO - Train Loss: 0.1225        \n",
      "2025-08-11 16:20:13,770 - INFO - Train Rmse: 0.3500\n",
      "2025-08-11 16:20:13,770 - INFO - Val   Loss: 0.1156\n",
      "2025-08-11 16:20:13,770 - INFO - Val   Rmse: 0.3400\n",
      "2025-08-11 16:20:13,770 - INFO - \n",
      "2025-08-11 16:20:13,772 - INFO - Epoch 9/20\n",
      "2025-08-11 16:20:55,099 - INFO - Train Loss: 0.1225        \n",
      "2025-08-11 16:20:55,100 - INFO - Train Rmse: 0.3499\n",
      "2025-08-11 16:20:55,100 - INFO - Val   Loss: 0.1157\n",
      "2025-08-11 16:20:55,100 - INFO - Val   Rmse: 0.3401\n",
      "2025-08-11 16:20:55,100 - INFO - \n",
      "2025-08-11 16:20:55,100 - INFO - Epoch 10/20\n",
      "2025-08-11 16:21:36,464 - INFO - Train Loss: 0.1224        \n",
      "2025-08-11 16:21:36,464 - INFO - Train Rmse: 0.3498\n",
      "2025-08-11 16:21:36,464 - INFO - Val   Loss: 0.1156\n",
      "2025-08-11 16:21:36,465 - INFO - Val   Rmse: 0.3400\n",
      "2025-08-11 16:21:36,465 - INFO - \n",
      "2025-08-11 16:21:36,466 - INFO - Epoch 11/20\n",
      "2025-08-11 16:22:17,815 - INFO - Train Loss: 0.1224        \n",
      "2025-08-11 16:22:17,816 - INFO - Train Rmse: 0.3497\n",
      "2025-08-11 16:22:17,816 - INFO - Val   Loss: 0.1157\n",
      "2025-08-11 16:22:17,816 - INFO - Val   Rmse: 0.3400\n",
      "2025-08-11 16:22:17,816 - INFO - \n",
      "2025-08-11 16:22:17,816 - INFO - Epoch 12/20\n",
      "2025-08-11 16:22:59,138 - INFO - Train Loss: 0.1223        \n",
      "2025-08-11 16:22:59,139 - INFO - Train Rmse: 0.3496\n",
      "2025-08-11 16:22:59,139 - INFO - Val   Loss: 0.1154\n",
      "2025-08-11 16:22:59,139 - INFO - Val   Rmse: 0.3397\n",
      "2025-08-11 16:22:59,139 - INFO - \n",
      "2025-08-11 16:22:59,141 - INFO - Epoch 13/20\n",
      "2025-08-11 16:23:40,530 - INFO - Train Loss: 0.1222        \n",
      "2025-08-11 16:23:40,531 - INFO - Train Rmse: 0.3496\n",
      "2025-08-11 16:23:40,531 - INFO - Val   Loss: 0.1156\n",
      "2025-08-11 16:23:40,531 - INFO - Val   Rmse: 0.3399\n",
      "2025-08-11 16:23:40,531 - INFO - \n",
      "2025-08-11 16:23:40,531 - INFO - Epoch 14/20\n",
      "2025-08-11 16:24:21,861 - INFO - Train Loss: 0.1222        \n",
      "2025-08-11 16:24:21,862 - INFO - Train Rmse: 0.3495\n",
      "2025-08-11 16:24:21,862 - INFO - Val   Loss: 0.1155\n",
      "2025-08-11 16:24:21,862 - INFO - Val   Rmse: 0.3398\n",
      "2025-08-11 16:24:21,862 - INFO - \n",
      "2025-08-11 16:24:21,862 - INFO - Epoch 15/20\n",
      "2025-08-11 16:25:03,188 - INFO - Train Loss: 0.1222        \n",
      "2025-08-11 16:25:03,189 - INFO - Train Rmse: 0.3495\n",
      "2025-08-11 16:25:03,189 - INFO - Val   Loss: 0.1156\n",
      "2025-08-11 16:25:03,190 - INFO - Val   Rmse: 0.3399\n",
      "2025-08-11 16:25:03,190 - INFO - \n",
      "2025-08-11 16:25:03,190 - INFO - Epoch 16/20\n",
      "2025-08-11 16:25:44,498 - INFO - Train Loss: 0.1221        \n",
      "2025-08-11 16:25:44,498 - INFO - Train Rmse: 0.3494\n",
      "2025-08-11 16:25:44,498 - INFO - Val   Loss: 0.1154\n",
      "2025-08-11 16:25:44,498 - INFO - Val   Rmse: 0.3397\n",
      "2025-08-11 16:25:44,499 - INFO - \n",
      "2025-08-11 16:25:44,499 - INFO - Epoch 17/20\n",
      "2025-08-11 16:26:25,791 - INFO - Train Loss: 0.1221        \n",
      "2025-08-11 16:26:25,791 - INFO - Train Rmse: 0.3494\n",
      "2025-08-11 16:26:25,792 - INFO - Val   Loss: 0.1155\n",
      "2025-08-11 16:26:25,792 - INFO - Val   Rmse: 0.3398\n",
      "2025-08-11 16:26:25,792 - INFO - \n",
      "2025-08-11 16:26:25,792 - INFO - Early stopping triggered at epoch 17\n"
     ]
    }
   ],
   "source": [
    "model, history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee085a5-1251-4816-84f2-f591c882b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectory_loader = TrajectoryDataset(X_train, next_return_train, spread_train, volatility_train, trajectory_length=16).as_dataloader(\n",
    "    batch_size=8, \n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_trajectory_loader = TrajectoryDataset(X_val, next_return_val, spread_val, volatility_val, trajectory_length=16).as_dataloader(\n",
    "    batch_size=8, \n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "test_trajectory_loader = TrajectoryDataset(X_test, next_return_test, spread_test, volatility_test, trajectory_length=16).as_dataloader(\n",
    "    batch_size=8, \n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041b148-50dc-4ca0-88cb-43761885bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnvironment(\n",
    "    reward_function=EstimatedReturnReward(fee=0.0, spread_multiplier=0.99),\n",
    ")\n",
    "\n",
    "backend = FullyConnectedBackend(\n",
    "    n_assets=len(config.data_config.symbol_or_symbols),\n",
    "    hidden_dim=128,\n",
    "    num_layers=2, \n",
    "    dropout=0.1,\n",
    "    use_layer_norm=False,\n",
    ")\n",
    "\n",
    "actor = RlActor(\n",
    "    model, \n",
    "    backend,\n",
    "    n_assets=len(config.data_config.symbol_or_symbols),\n",
    "    train_signal_predictor=False, \n",
    "    exploration_eps=0.0\n",
    ").to(device)\n",
    "\n",
    "signal_predictor_actor = SignalPredictorActor(\n",
    "    model, \n",
    "    trade_asset_count=1,\n",
    "    train_signal_predictor=False\n",
    ").to(device)\n",
    "\n",
    "rl_agent = RlAgent(\n",
    "    actor, \n",
    "    env,\n",
    "    single_action_per_trajectory=False\n",
    ")\n",
    "\n",
    "metrics_calculator = MetricsCalculator(\n",
    "    metrics=DEFAULT_METRICS\n",
    ")\n",
    "\n",
    "policy_gradient = PolicyGradient(\n",
    "    rl_agent, \n",
    "    train_trajectory_loader, \n",
    "    val_trajectory_loader, \n",
    "    metrics_calculator=metrics_calculator,\n",
    "    optimizer=torch.optim.AdamW(\n",
    "        [p for p in actor.parameters() if p.requires_grad], \n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        amsgrad=True),\n",
    "    scheduler=None,\n",
    "    loss_fn=SumLogReturnLoss(use_baseline=False),\n",
    "    num_epochs=10,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8dec5-eaf9-4f91-af35-30ea24c0e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 16:32:01,912 - INFO - [PolicyGradient] [VAL] Epoch 0/10 — CumulativeReturn: 0.4364, MeanReturnPercentage: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PolicyGradient] [VAL] Epoch 0/10 — Loss: -0.0453\n"
     ]
    }
   ],
   "source": [
    "print('Val set evaluation')\n",
    "epoch_loss, realized_returns_signal_predictor, actions_signal_predictor = policy_gradient.evaluate(signal_predictor_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set evaluation')\n",
    "epoch_loss, realized_returns_signal_predictor, actions_signal_predictor = policy_gradient.evaluate(signal_predictor_actor, test_trajectory_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c7dcead-f58a-4306-84ab-9ca83cfb332c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't subtract offset-naive and offset-aware datetimes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_cumulative_wealth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturns_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSignal Predictor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealized_returns_signal_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_set_last_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/intraday-portfolio-management/modeling/rl/visualization/wealth_plot.py:10\u001b[39m, in \u001b[36mplot_cumulative_wealth\u001b[39m\u001b[34m(returns_dict, start_time, end_time)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_cumulative_wealth\u001b[39m(returns_dict: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]], start_time: datetime, end_time: datetime):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Create uniform datetime range\u001b[39;00m\n\u001b[32m      8\u001b[39m     n_points = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(returns_dict.values())))  \u001b[38;5;66;03m# get length from any series\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     time_points = \u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Initial wealth\u001b[39;00m\n\u001b[32m     16\u001b[39m     initial_wealth = \u001b[32m1.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/intraday-portfolio-management/modeling/rl/visualization/wealth_plot.py:11\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_cumulative_wealth\u001b[39m(returns_dict: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]], start_time: datetime, end_time: datetime):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Create uniform datetime range\u001b[39;00m\n\u001b[32m      8\u001b[39m     n_points = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(returns_dict.values())))  \u001b[38;5;66;03m# get length from any series\u001b[39;00m\n\u001b[32m     10\u001b[39m     time_points = [\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         start_time + i * (\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m) / (n_points - \u001b[32m1\u001b[39m)\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_points)\n\u001b[32m     13\u001b[39m     ]\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Initial wealth\u001b[39;00m\n\u001b[32m     16\u001b[39m     initial_wealth = \u001b[32m1.0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: can't subtract offset-naive and offset-aware datetimes"
     ]
    }
   ],
   "source": [
    "plot_cumulative_wealth(\n",
    "    returns_dict={\n",
    "        'Signal Predictor': realized_returns_signal_predictor,\n",
    "    }, \n",
    "    start_time=config.data_config.train_set_last_date, \n",
    "    end_time=config.data_config.end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9955c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy  # Local import to avoid polluting global namespace unnecessarily\n",
    "state_dict = (\n",
    "    model.module.state_dict()\n",
    "        if isinstance(model, torch.nn.DataParallel)\n",
    "    else model.state_dict()\n",
    ")\n",
    "\n",
    "# Keep a local copy of the best weights so we can return the best model\n",
    "# after training finishes, without needing to reload from disk.\n",
    "best_model_state = copy.deepcopy(state_dict)\n",
    "\n",
    "# Persist to disk if a save_path was provided\n",
    "torch.save(state_dict, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed829636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/26 15:35:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'LSTM Default' already exists. Creating a new version of this model...\n",
      "2025/06/26 15:35:14 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LSTM Default, version 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run gentle-loon-699 at: http://127.0.0.1:8080/#/experiments/439216085822475480/runs/54deb1104660468d9ffb4e7e278e9cfb\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/439216085822475480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '10' of model 'LSTM Default'.\n"
     ]
    }
   ],
   "source": [
    "log_experiment(\n",
    "    config=config, \n",
    "    model=model, \n",
    "    history=history,\n",
    "    input_data_sample=next(iter(train_loader))[0].to(trainer.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c286205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9435\n",
      "[LightGBM] [Info] Number of data points in the train set: 7371, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.497863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train rmse: 0.26411260601695974, Test rmse: 0.2684210886033184, Baseline rmse: 0.2599985897541046\n",
      "Expected return: 0.00010183148393891163, Baseline return: 2.569958041931386e-06, Max possible return 0.00048079571570269763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluate_lgb_regressor(X_train, y_train, X_val, y_val, next_return_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
