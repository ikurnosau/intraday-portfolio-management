{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a6dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime, timezone\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format for the log messages\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Log to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from data.raw.retrievers.alpaca_markets_retriever import AlpacaMarketsRetriever\n",
    "from config.constants import *\n",
    "from data.processed.dataset_creation import DatasetCreator\n",
    "from data.processed.indicators import *\n",
    "from data.processed.targets import Balanced3ClassClassification\n",
    "from data.processed.normalization import ZScoreOverWindowNormalizer, ZScoreNormalizer, MinMaxNormalizer\n",
    "from data.processed.dataset_pytorch import DatasetPytorch\n",
    "from modeling.trainer import Trainer\n",
    "from modeling.evaluate import evaluate_lgb_regressor, evaluate_torch_regressor, evaluate_torch_regressor_multiasset\n",
    "from observability.mlflow_integration import log_experiment\n",
    "\n",
    "from config.experiments.cur_experiment import config\n",
    "\n",
    "torch.backends.cudnn.benchmark = config.train_config.cudnn_benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "755a729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1On6h2pn05svQFj20gU_iyCFuWGwhEYPk\n",
      "From (redirected): https://drive.google.com/uc?id=1On6h2pn05svQFj20gU_iyCFuWGwhEYPk&confirm=t&uuid=ce999821-8544-4a6a-b0e3-a38cf22bfd90\n",
      "To: c:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\data\\raw\\alpaca\\temp\\1Min_2024-06-01-2025-06-01_AAPL+MSFT+NVDA+GOOGL+GOOG+META+AVGO+AMD+TSM+QCOM+ORCL+INTC+CSCO+IBM+MU+ADBE+TXN+CRM+PANW+AMAT+SQ+PYP.pkl\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 353M/353M [00:13<00:00, 26.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "retriever = AlpacaMarketsRetriever(download_from_gdrive=False)\n",
    "\n",
    "retrieval_result = retriever.bars(\n",
    "    symbol_or_symbols=config.data_config.symbol_or_symbols, \n",
    "    start=config.data_config.start, \n",
    "    end=config.data_config.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d48d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 15:03:37,359 - INFO - Processing AAPL ‚Ä¶\n",
      "2025-07-02 15:03:37,680 - INFO - Imputing 496 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:39,444 - INFO - Processing ADBE ‚Ä¶\n",
      "2025-07-02 15:03:39,665 - INFO - Imputing 5392 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:41,505 - INFO - Processing ADI ‚Ä¶\n",
      "2025-07-02 15:03:41,732 - INFO - Imputing 6204 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:43,449 - INFO - Processing AMAT ‚Ä¶\n",
      "2025-07-02 15:03:43,677 - INFO - Imputing 4035 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:45,417 - INFO - Processing AMD ‚Ä¶\n",
      "2025-07-02 15:03:45,669 - INFO - Imputing 214 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:47,430 - INFO - Processing ANET ‚Ä¶\n",
      "2025-07-02 15:03:47,637 - INFO - Imputing 5097 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:49,404 - INFO - Processing AVGO ‚Ä¶\n",
      "2025-07-02 15:03:49,657 - INFO - Imputing 1059 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:51,433 - INFO - Processing CDNS ‚Ä¶\n",
      "2025-07-02 15:03:51,670 - INFO - Imputing 9038 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:53,431 - INFO - Processing CRM ‚Ä¶\n",
      "2025-07-02 15:03:53,658 - INFO - Imputing 3774 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:55,462 - INFO - Processing CRWD ‚Ä¶\n",
      "2025-07-02 15:03:55,750 - INFO - Imputing 4120 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:57,791 - INFO - Processing CSCO ‚Ä¶\n",
      "2025-07-02 15:03:58,037 - INFO - Imputing 3929 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:03:59,908 - INFO - Processing DDOG ‚Ä¶\n",
      "2025-07-02 15:04:00,150 - INFO - Imputing 4855 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:01,938 - INFO - Processing DELL ‚Ä¶\n",
      "2025-07-02 15:04:02,376 - INFO - Imputing 2432 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:04,134 - INFO - Processing FTNT ‚Ä¶\n",
      "2025-07-02 15:04:04,356 - INFO - Imputing 4760 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:06,174 - INFO - Processing GOOGL ‚Ä¶\n",
      "2025-07-02 15:04:06,425 - INFO - Imputing 877 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:08,182 - INFO - Processing GOOG ‚Ä¶\n",
      "2025-07-02 15:04:08,417 - INFO - Imputing 1161 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:10,188 - INFO - Processing HPQ ‚Ä¶\n",
      "2025-07-02 15:04:10,386 - INFO - Imputing 5057 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:12,107 - INFO - Processing IBM ‚Ä¶\n",
      "2025-07-02 15:04:12,323 - INFO - Imputing 5038 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:14,062 - INFO - Processing INTC ‚Ä¶\n",
      "2025-07-02 15:04:14,327 - INFO - Imputing 215 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:16,090 - INFO - Processing INTU ‚Ä¶\n",
      "2025-07-02 15:04:16,310 - INFO - Imputing 12312 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:18,057 - INFO - Processing KLAC ‚Ä¶\n",
      "2025-07-02 15:04:18,244 - INFO - Imputing 22343 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:20,050 - INFO - Processing LRCX ‚Ä¶\n",
      "2025-07-02 15:04:20,266 - INFO - Imputing 8026 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:22,050 - INFO - Processing MCHP ‚Ä¶\n",
      "2025-07-02 15:04:22,266 - INFO - Imputing 4728 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:24,064 - INFO - Processing MDB ‚Ä¶\n",
      "2025-07-02 15:04:24,281 - INFO - Imputing 9730 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:26,035 - INFO - Processing META ‚Ä¶\n",
      "2025-07-02 15:04:26,268 - INFO - Imputing 1465 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:28,302 - INFO - Processing MRVL ‚Ä¶\n",
      "2025-07-02 15:04:28,561 - INFO - Imputing 2386 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:30,397 - INFO - Processing MSFT ‚Ä¶\n",
      "2025-07-02 15:04:30,648 - INFO - Imputing 1493 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:32,511 - INFO - Processing MU ‚Ä¶\n",
      "2025-07-02 15:04:32,779 - INFO - Imputing 838 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:34,560 - INFO - Processing NET ‚Ä¶\n",
      "2025-07-02 15:04:34,811 - INFO - Imputing 5550 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:36,593 - INFO - Processing NOW ‚Ä¶\n",
      "2025-07-02 15:04:36,832 - INFO - Imputing 13543 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:38,663 - INFO - Processing NVDA ‚Ä¶\n",
      "2025-07-02 15:04:38,968 - INFO - Imputing 1 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:40,779 - INFO - Processing NXPI ‚Ä¶\n",
      "2025-07-02 15:04:41,018 - INFO - Imputing 7743 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:42,833 - INFO - Processing OKTA ‚Ä¶\n",
      "2025-07-02 15:04:43,085 - INFO - Imputing 6551 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:44,882 - INFO - Processing ON ‚Ä¶\n",
      "2025-07-02 15:04:45,115 - INFO - Imputing 4325 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:46,902 - INFO - Processing ORCL ‚Ä¶\n",
      "2025-07-02 15:04:47,136 - INFO - Imputing 3440 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:49,041 - INFO - Processing PANW ‚Ä¶\n",
      "2025-07-02 15:04:49,296 - INFO - Imputing 4966 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:51,096 - INFO - Processing PLTR ‚Ä¶\n",
      "2025-07-02 15:04:51,569 - INFO - Imputing 58 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:53,353 - INFO - Processing PYPL ‚Ä¶\n",
      "2025-07-02 15:04:53,594 - INFO - Imputing 3097 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:55,513 - INFO - Processing QCOM ‚Ä¶\n",
      "2025-07-02 15:04:55,760 - INFO - Imputing 3487 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:57,584 - INFO - Processing SHOP ‚Ä¶\n",
      "2025-07-02 15:04:57,814 - INFO - Imputing 3314 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:04:59,646 - INFO - Processing SMCI ‚Ä¶\n",
      "2025-07-02 15:04:59,918 - INFO - Imputing 242 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:05:01,748 - INFO - Processing SNOW ‚Ä¶\n",
      "2025-07-02 15:05:02,013 - INFO - Imputing 3001 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:05:03,864 - INFO - Processing SNPS ‚Ä¶\n",
      "2025-07-02 15:05:04,101 - INFO - Imputing 18331 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:05:05,903 - INFO - Processing SQ ‚Ä¶\n",
      "2025-07-02 15:05:06,067 - INFO - Imputing 1647 NaN rows out of 61778 with forward fill..\n",
      "2025-07-02 15:05:07,234 - INFO - SQ has 55457 rows, but 87398 are expected. Skipping ‚Ä¶\n",
      "2025-07-02 15:05:07,234 - INFO - Processing STX ‚Ä¶\n",
      "2025-07-02 15:05:07,484 - INFO - Imputing 6823 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:05:09,320 - INFO - Processing TEAM ‚Ä¶\n",
      "2025-07-02 15:05:09,552 - INFO - Imputing 8239 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:05:11,376 - INFO - Processing TSM ‚Ä¶\n",
      "2025-07-02 15:05:11,621 - INFO - Imputing 1209 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:05:13,426 - INFO - Processing TXN ‚Ä¶\n",
      "2025-07-02 15:05:13,656 - INFO - Imputing 5080 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:05:15,460 - INFO - Processing WDC ‚Ä¶\n",
      "2025-07-02 15:05:15,678 - INFO - Imputing 5012 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:05:17,506 - INFO - Processing ZS ‚Ä¶\n",
      "2025-07-02 15:05:17,776 - INFO - Imputing 8329 NaN rows out of 97359 with forward fill..\n",
      "2025-07-02 15:05:19,600 - INFO - Finished feature generation. 1 assets skipped due to insufficient rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((49, 79999, 30, 37),\n",
       " (49, 79999),\n",
       " (49, 79999),\n",
       " (49, 7341, 30, 37),\n",
       " (49, 7341),\n",
       " (49, 7341))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_creator = DatasetCreator(\n",
    "    features=config.data_config.features,\n",
    "    target=config.data_config.target,\n",
    "    normalizer=config.data_config.normalizer,\n",
    "    missing_values_handler=config.data_config.missing_values_handler,\n",
    "    train_set_last_date=config.data_config.train_set_last_date, \n",
    "    in_seq_len=config.data_config.in_seq_len,\n",
    "    multi_asset_prediction=config.data_config.multi_asset_prediction,\n",
    ")\n",
    "\n",
    "X_train, y_train, next_return_train, X_test, y_test, next_return_test = dataset_creator.create_dataset_numpy(retrieval_result)\n",
    "X_train.shape, y_train.shape, next_return_train.shape, X_test.shape, y_test.shape, next_return_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f50717e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79999, 49, 30, 37),\n",
       " (79999, 49),\n",
       " (79999, 49),\n",
       " (7341, 49, 30, 37),\n",
       " (7341, 49),\n",
       " (7341, 49))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.data_config.multi_asset_prediction:\n",
    "    X_train = np.swapaxes(X_train, 0, 1)\n",
    "    y_train = np.swapaxes(y_train, 0, 1)\n",
    "    next_return_train = np.swapaxes(next_return_train, 0, 1)\n",
    "\n",
    "    X_test = np.swapaxes(X_test, 0, 1)\n",
    "    y_test = np.swapaxes(y_test, 0, 1)\n",
    "    next_return_test = np.swapaxes(next_return_test, 0, 1)\n",
    "\n",
    "X_train.shape, y_train.shape, next_return_train.shape, X_test.shape, y_test.shape, next_return_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc91696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.50199676, 0.5025806)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98a4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetPytorch(X_train, y_train, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")\n",
    "test_loader = DatasetPytorch(X_test, y_test, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4b5aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalSpatial(\n",
       "  (asset_embed): Embedding(49, 16)\n",
       "  (asset_proj): Linear(in_features=16, out_features=128, bias=False)\n",
       "  (lstm): LSTM(37, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (spatial_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = config.model_config.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3858fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    loss_fn=config.train_config.loss_fn,\n",
    "    optimizer=config.train_config.optimizer,\n",
    "    scheduler=config.train_config.scheduler,\n",
    "    num_epochs=config.train_config.num_epochs,\n",
    "    device=config.train_config.device,\n",
    "    metrics=config.train_config.metrics,\n",
    "    save_path=config.train_config.save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01c52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 15:09:42,624 - INFO - Epoch 1/100\n",
      "2025-07-02 15:10:26,403 - INFO - Train Loss: 0.1246           \n",
      "2025-07-02 15:10:26,404 - INFO - Train Rmse: 0.3528\n",
      "2025-07-02 15:10:26,404 - INFO - Val   Loss: 0.1094\n",
      "2025-07-02 15:10:26,404 - INFO - Val   Rmse: 0.3305\n",
      "2025-07-02 15:10:26,404 - INFO - \n",
      "2025-07-02 15:10:26,405 - INFO - Epoch 2/100\n",
      "2025-07-02 15:11:21,429 - INFO - Train Loss: 0.1238           \n",
      "2025-07-02 15:11:21,429 - INFO - Train Rmse: 0.3516\n",
      "2025-07-02 15:11:21,430 - INFO - Val   Loss: 0.1098\n",
      "2025-07-02 15:11:21,430 - INFO - Val   Rmse: 0.3311\n",
      "2025-07-02 15:11:21,431 - INFO - \n",
      "2025-07-02 15:11:21,431 - INFO - Epoch 3/100\n",
      "2025-07-02 15:12:40,972 - INFO - Train Loss: 0.1234          \n",
      "2025-07-02 15:12:40,973 - INFO - Train Rmse: 0.3511\n",
      "2025-07-02 15:12:40,973 - INFO - Val   Loss: 0.1094\n",
      "2025-07-02 15:12:40,974 - INFO - Val   Rmse: 0.3305\n",
      "2025-07-02 15:12:40,974 - INFO - \n",
      "2025-07-02 15:12:40,975 - INFO - Epoch 4/100\n",
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\trainer.py:60\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     58\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     val_loss, val_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\trainer.py:114\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 114\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3802d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([127, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([1024, 49, 30, 37])\n",
      "torch.Size([173, 49, 30, 37])\n",
      "Train rmse: 0.3522928059101105, Test rmse: 0.3322025537490845, Baseline rmse: 0.3323450982570648\n",
      "Expected return: 1.8094615874559525e-05, Baseline return: 7.885851118771825e-06, Max possible return 0.0005796058103442192\n"
     ]
    }
   ],
   "source": [
    "evaluate_torch_regressor_multiasset(model, X_train, y_train, X_test, y_test, next_return_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed829636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/26 15:35:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'LSTM Default' already exists. Creating a new version of this model...\n",
      "2025/06/26 15:35:14 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LSTM Default, version 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run gentle-loon-699 at: http://127.0.0.1:8080/#/experiments/439216085822475480/runs/54deb1104660468d9ffb4e7e278e9cfb\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/439216085822475480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '10' of model 'LSTM Default'.\n"
     ]
    }
   ],
   "source": [
    "log_experiment(\n",
    "    config=config, \n",
    "    model=model, \n",
    "    history=history,\n",
    "    input_data_sample=next(iter(train_loader))[0].to(trainer.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c286205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9435\n",
      "[LightGBM] [Info] Number of data points in the train set: 7371, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.497863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train rmse: 0.26411260601695974, Test rmse: 0.2684210886033184, Baseline rmse: 0.2599985897541046\n",
      "Expected return: 0.00010183148393891163, Baseline return: 2.569958041931386e-06, Max possible return 0.00048079571570269763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluate_lgb_regressor(X_train, y_train, X_test, y_test, next_return_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
