{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3a6dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime, timezone\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format for the log messages\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Log to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from core_data_prep.core_data_prep import DataPreparer\n",
    "from core_data_prep.validations import Validator\n",
    "\n",
    "from data.raw.retrievers.alpaca_markets_retriever import AlpacaMarketsRetriever\n",
    "from data.raw.retrievers.stooq_retriever import StooqRetriever\n",
    "from config.constants import *\n",
    "from data.processed.dataset_creation import DatasetCreator\n",
    "from data.processed.indicators import *\n",
    "from data.processed.targets import Balanced3ClassClassification\n",
    "from data.processed.normalization import ZScoreOverWindowNormalizer, ZScoreNormalizer, MinMaxNormalizer\n",
    "from data.processed.dataset_pytorch import DatasetPytorch\n",
    "from modeling.trainer import Trainer\n",
    "from modeling.evaluate import evaluate_lgb_regressor, evaluate_torch_regressor, evaluate_torch_regressor_multiasset\n",
    "\n",
    "from modeling.rl.environment import PortfolioEnvironment\n",
    "from modeling.rl.state import State\n",
    "from modeling.rl.agent import RlAgent\n",
    "from modeling.rl.algorithms.policy_gradient import PolicyGradient\n",
    "from modeling.rl.actors.actor import RlActor, FullyConnectedBackend, TransformerBackend\n",
    "from modeling.rl.actors.signal_predictor_actor import SignalPredictorActor\n",
    "from modeling.rl.actors.high_energy_low_friction_actor import HighEnergyLowFrictionActor\n",
    "from modeling.rl.actors.xsmom_actor import XSMomActor\n",
    "from modeling.rl.actors.tsmom_actor import TSMomActor\n",
    "from modeling.rl.actors.blsw_actor import BLSWActor\n",
    "from modeling.rl.trajectory_dataset import TrajectoryDataset\n",
    "from modeling.rl.metrics import MetricsCalculator, DEFAULT_METRICS\n",
    "from modeling.rl.reward import EstimatedReturnReward\n",
    "from modeling.rl.loss import SumLogReturnLoss\n",
    "from modeling.rl.visualization.wealth_plot import plot_cumulative_wealth\n",
    "from modeling.rl.visualization.position_plot import plot_position_heatmap\n",
    "from config.experiments.cur_experiment import config\n",
    "\n",
    "torch.backends.cudnn.benchmark = config.train_config.cudnn_benchmark\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755a729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = AlpacaMarketsRetriever(download_from_gdrive=False, timeframe=config.data_config.frequency)\n",
    "\n",
    "# retrieval_result = retriever.bars_with_quotes(\n",
    "#     symbol_or_symbols=config.data_config.symbol_or_symbols, \n",
    "#     start=config.data_config.start, \n",
    "#     end=config.data_config.end)\n",
    "\n",
    "# retrieval_result = retriever.bars_with_quotes(\n",
    "#     symbol_or_symbols=['AAPL', 'XLY'], \n",
    "#     start=config.data_config.start, \n",
    "#     end=config.data_config.end)\n",
    "\n",
    "retriever = StooqRetriever(download_from_gdrive=False)\n",
    "retrieval_result = retriever.bars(start=config.data_config.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c39000dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparer = DataPreparer(\n",
    "    normalizer=config.data_config.normalizer,\n",
    "    missing_values_handler=config.data_config.missing_values_handler,\n",
    "    in_seq_len=config.data_config.in_seq_len,\n",
    "    frequency=str(config.data_config.frequency),\n",
    "    validator=Validator(),\n",
    "    backend='threading'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2313bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 12:11:15,832 - INFO - Using monolithic slices with -60 timestamps\n",
      "2025-11-07 12:11:16,255 - INFO - Found 7393 train slices, 504 val slices, 754 test slices\n",
      "2025-11-07 12:11:16,255 - INFO - Trained per-asset targets\n",
      "2025-11-07 12:11:16,446 - INFO - Input data validated!\n",
      "2025-11-07 12:11:20,000 - INFO - Filled data validated!\n",
      "2025-11-07 12:11:22,921 - INFO - Normalised features validated!\n",
      "2025-11-07 12:11:23,448 - INFO - X validated!\n",
      "2025-11-07 12:11:23,597 - INFO - Target mean: 0.5040530562400818\n",
      "2025-11-07 12:11:23,598 - INFO - Target validated!\n",
      "2025-11-07 12:11:23,641 - INFO - Statistics 'next_return' validated!\n",
      "2025-11-07 12:11:23,699 - INFO - Statistics 'volatility' validated!\n",
      "2025-11-07 12:11:23,736 - INFO - Statistics 'spread' validated!\n",
      "2025-11-07 12:11:24,367 - INFO - Input data validated!\n",
      "2025-11-07 12:11:25,230 - INFO - Filled data validated!\n",
      "2025-11-07 12:11:26,098 - INFO - Normalised features validated!\n",
      "2025-11-07 12:11:26,125 - INFO - X validated!\n",
      "2025-11-07 12:11:26,154 - INFO - Target mean: 0.510819673538208\n",
      "2025-11-07 12:11:26,155 - INFO - Target validated!\n",
      "2025-11-07 12:11:26,208 - INFO - Statistics 'next_return' validated!\n",
      "2025-11-07 12:11:26,275 - INFO - Statistics 'volatility' validated!\n",
      "2025-11-07 12:11:26,323 - INFO - Statistics 'spread' validated!\n",
      "2025-11-07 12:11:26,460 - INFO - Input data validated!\n",
      "2025-11-07 12:11:26,947 - INFO - Filled data validated!\n",
      "2025-11-07 12:11:27,842 - INFO - Normalised features validated!\n",
      "2025-11-07 12:11:27,877 - INFO - X validated!\n",
      "2025-11-07 12:11:27,905 - INFO - Target mean: 0.522655725479126\n",
      "2025-11-07 12:11:27,905 - INFO - Target validated!\n",
      "2025-11-07 12:11:27,938 - INFO - Statistics 'next_return' validated!\n",
      "2025-11-07 12:11:27,975 - INFO - Statistics 'volatility' validated!\n",
      "2025-11-07 12:11:28,000 - INFO - Statistics 'spread' validated!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((15951, 30, 60, 16),\n",
       " (15951, 30),\n",
       " (15951, 30),\n",
       " (610, 30, 60, 16),\n",
       " (610, 30),\n",
       " (610, 30),\n",
       " (974, 30, 60, 16),\n",
       " (974, 30),\n",
       " (974, 30))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train, statistics_train), (X_val, y_val, statistics_val), (X_test, y_test, statistics_test) = \\\n",
    "    data_preparer.get_experiment_data(\n",
    "        data=retrieval_result,\n",
    "        start_date=config.data_config.start,\n",
    "        end_date=config.data_config.end,\n",
    "        features=config.data_config.features,\n",
    "        statistics=config.data_config.statistics,\n",
    "        target=config.data_config.target,\n",
    "        train_set_last_date=config.data_config.train_set_last_date,\n",
    "        val_set_last_date=config.data_config.val_set_last_date,\n",
    "    )\n",
    "\n",
    "X_train.shape, y_train.shape, statistics_train['next_return'].shape, \\\n",
    "    X_val.shape, y_val.shape, statistics_val['next_return'].shape, \\\n",
    "    X_test.shape, y_test.shape, statistics_test['next_return'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313bc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run legendary-pug-535 at: http://127.0.0.1:8080/#/experiments/709387658771016215/runs/1b3a928808344896b5d97c092ed6b08f\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/709387658771016215\n"
     ]
    }
   ],
   "source": [
    "# from observability.mlflow_integration import log_experiment\n",
    "\n",
    "\n",
    "# log_experiment(\n",
    "#     config=config, \n",
    "#     validator_snapshots=data_preparer.validator.snapshots\n",
    "#     # model=model, \n",
    "#     # history=history,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fca6e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_return_train, spread_train, volatility_train, \\\n",
    "    next_return_val, spread_val, volatility_val, \\\n",
    "    next_return_test, spread_test, volatility_test = \\\n",
    "        statistics_train['next_return'], statistics_train['spread'], statistics_train['volatility'], \\\n",
    "        statistics_val['next_return'], statistics_val['spread'], statistics_val['volatility'], \\\n",
    "        statistics_test['next_return'], statistics_test['spread'], statistics_test['volatility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1dfe485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5 , 0.5 , 0.5 , ..., 0.5 , 0.5 , 0.75],\n",
       "       [0.5 , 0.5 , 0.5 , ..., 0.5 , 0.5 , 0.75],\n",
       "       [0.5 , 0.5 , 0.5 , ..., 0.5 , 0.5 , 0.75],\n",
       "       ...,\n",
       "       [0.25, 0.5 , 0.25, ..., 0.75, 0.5 , 0.75],\n",
       "       [0.75, 0.25, 0.75, ..., 0.75, 0.5 , 0.75],\n",
       "       [1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c0fcb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_slices = data_preparer._get_daily_slices(retrieval_result, config.data_config.start, config.data_config.end, \n",
    "#     Constants.Data.TRADING_DAY_LENGTH_MINUTES  + data_preparer.in_seq_len + data_preparer.normalizer.window + 30)\n",
    "\n",
    "# per_asset_target = data_preparer._train_target_per_asset(\n",
    "#             config.data_config.target,\n",
    "#             daily_slices,\n",
    "#             n_timestamps_per_slice=Constants.Data.TRADING_DAY_LENGTH_MINUTES\n",
    "#         )\n",
    "\n",
    "# X, y, statistics = data_preparer.transform_data_for_inference( \n",
    "#                                       data=daily_slices[1],\n",
    "#                                       n_timestamps=Constants.Data.TRADING_DAY_LENGTH_MINUTES,\n",
    "#                                       features=config.data_config.features,\n",
    "#                                       include_target_and_statistics=True,\n",
    "#                                       statistics=config.data_config.statistics,\n",
    "#                                       per_asset_target=per_asset_target,\n",
    "#                                       n_jobs=os.cpu_count() // 2\n",
    "#                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18d48d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_creator = DatasetCreator(\n",
    "#     features=config.data_config.features,\n",
    "#     target=config.data_config.target,\n",
    "#     normalizer=config.data_config.normalizer,\n",
    "#     missing_values_handler=config.data_config.missing_values_handler,\n",
    "#     train_set_last_date=config.data_config.train_set_last_date, \n",
    "#     val_set_last_date=config.data_config.val_set_last_date,\n",
    "#     cutoff_time=config.data_config.cutoff_time,\n",
    "#     in_seq_len=config.data_config.in_seq_len,\n",
    "#     multi_asset_prediction=config.data_config.multi_asset_prediction,\n",
    "# )\n",
    "\n",
    "# X_train, y_train, next_return_train, spread_train, volatility_train, \\\n",
    "#     X_val, y_val, next_return_val, spread_val, volatility_val, \\\n",
    "#     X_test, y_test, next_return_test, spread_test, volatility_test \\\n",
    "#         = dataset_creator.create_dataset_numpy(retrieval_result)\n",
    "\n",
    "# X_train.shape, y_train.shape, next_return_train.shape, spread_train.shape, volatility_train.shape, \\\n",
    "#     X_val.shape, y_val.shape, next_return_val.shape, spread_val.shape, volatility_val.shape, \\\n",
    "#         X_test.shape, y_test.shape, next_return_test.shape, spread_test.shape, volatility_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85079cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \t    next_return, spread, volatility\n",
    "# train (0.00062720885, 0.00025727754, 0.000789116)\n",
    "# val   (0.0011397429, 0.0002572774, 0.0014156421)\n",
    "# test  (0.0005497588, 0.0002572774, 0.00068560627)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e29a11c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0072778086, 0.0, 0.011016902)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(next_return_train).mean(), spread_train.mean(), volatility_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d798023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0065201996, 0.0, 0.009838276)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(next_return_val).mean(), spread_val.mean(), volatility_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "677eea8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00623178, 0.0, 0.009423372)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(next_return_test).mean(), spread_test.mean(), volatility_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b98a4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetPytorch(X_train, y_train, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")\n",
    "val_loader = DatasetPytorch(X_val, y_val, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c4b5aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalSpatial(\n",
       "  (asset_embed): Embedding(50, 16)\n",
       "  (asset_proj): Linear(in_features=16, out_features=256, bias=False)\n",
       "  (lstm): LSTM(16, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (spatial_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = config.model_config.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fa26ca6-8128-4e85-be01-34335ab51675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentConfig(data_config=DataConfig(symbol_or_symbols=['AAPL', 'AMD', 'BABA', 'BITU', 'C', 'CSCO', 'DAL', 'DIA', 'GLD', 'GOOG', 'IJR', 'MARA', 'MRVL', 'MU', 'NEE', 'NKE', 'NVDA', 'ON', 'PLTR', 'PYPL', 'QLD', 'QQQ', 'QQQM', 'RKLB', 'RSP', 'SMCI', 'SMH', 'SOXL', 'SOXX', 'SPXL', 'SPY', 'TMF', 'TNA', 'TQQQ', 'TSLA', 'UBER', 'UDOW', 'UPRO', 'VOO', 'WFC', 'XBI', 'XLC', 'XLE', 'XLI', 'XLK', 'XLU', 'XLV', 'XLY', 'XOM', 'XRT'], frequency=<alpaca.data.timeframe.TimeFrame object at 0x000001C3557A6FF0>, start=datetime.datetime(1970, 1, 2, 0, 0, tzinfo=zoneinfo.ZoneInfo(key='America/New_York')), end=datetime.datetime(2019, 1, 2, 0, 0, tzinfo=zoneinfo.ZoneInfo(key='America/New_York')), train_set_last_date=datetime.datetime(2014, 1, 1, 0, 0, tzinfo=zoneinfo.ZoneInfo(key='America/New_York')), val_set_last_date=datetime.datetime(2016, 1, 1, 0, 0, tzinfo=zoneinfo.ZoneInfo(key='America/New_York')), features={'log_ret': <function <lambda> at 0x000001C35309EF20>, 'hl_range': <function <lambda> at 0x000001C3558B63E0>, 'close_open': <function <lambda> at 0x000001C3558B6980>, 'vol_delta': <function <lambda> at 0x000001C3558B6A20>, 'EMA_fast': <data.processed.indicators.EMA object at 0x000001C3558B9790>, 'EMA_slow': <data.processed.indicators.EMA object at 0x000001C3558B97C0>, 'RSI2': <data.processed.indicators.RSI object at 0x000001C3558B97F0>, 'RSI6': <data.processed.indicators.RSI object at 0x000001C35402EC90>, 'realvol20': <function <lambda> at 0x000001C3558B6AC0>, 'VWAP_dist': <function <lambda> at 0x000001C3558B6B60>, 'loc_in_range': <function <lambda> at 0x000001C3558B6C00>, 'tod_sin': <function <lambda> at 0x000001C3558B6CA0>, 'tod_cos': <function <lambda> at 0x000001C3558B6D40>, 'ema_slope': <function <lambda> at 0x000001C3558B6DE0>, 'vol_slope': <function <lambda> at 0x000001C3558B6E80>, 'is_missing': <function <lambda> at 0x000001C3558B6F20>}, statistics={'next_return': <function <lambda> at 0x000001C3558B6FC0>, 'volatility': <function <lambda> at 0x000001C3558B7060>, 'spread': <function <lambda> at 0x000001C3558B7100>}, target=<data.processed.targets.FutureMeanReturnClassification object at 0x000001C355847260>, normalizer=<data.processed.normalization.MinMaxNormalizerOverWindow object at 0x000001C3558B9820>, missing_values_handler=<core_data_prep.core_data_prep.ContinuousForwardFill object at 0x000001C3543B5CA0>, in_seq_len=60, multi_asset_prediction=True, cutoff_time=datetime.time(10, 0)), model_config=ModelConfig(model=TemporalSpatial(\n",
       "  (asset_embed): Embedding(50, 16)\n",
       "  (asset_proj): Linear(in_features=16, out_features=256, bias=False)\n",
       "  (lstm): LSTM(16, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (spatial_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "), registered_model_name='TemporalSpatial Regressor'), train_config=TrainConfig(loss_fn=MSELoss(), optimizer=AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: True\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 1e-10\n",
       "), scheduler={'type': 'OneCycleLR', 'max_lr': 0.003, 'pct_start': 0.1, 'div_factor': 25, 'final_div_factor': 1000.0, 'anneal_strategy': 'cos', 'cycle_momentum': False}, num_epochs=20, early_stopping_patience=10, device=device(type='cuda'), cudnn_benchmark=True, metrics={'rmse': <function rmse_regression at 0x000001C3558B6480>}, batch_size=128, shuffle=True, num_workers=8, prefetch_factor=4, pin_memory=True, persistent_workers=True, drop_last=True, save_path=''), observability_config=ObservabilityConfig(experiment_name='Return Regression MLP'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a3858fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=config.train_config.loss_fn,\n",
    "    optimizer=config.train_config.optimizer,\n",
    "    scheduler=config.train_config.scheduler,\n",
    "    num_epochs=config.train_config.num_epochs,\n",
    "    early_stopping_patience=config.train_config.early_stopping_patience,\n",
    "    device=config.train_config.device,\n",
    "    metrics=config.train_config.metrics,\n",
    "    save_path=config.train_config.save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1da6d315-3bf8-452a-ba17-6badd562dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1154\n",
    "# 0.3397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f01c52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 11:03:10,350 - INFO - Epoch 1/20\n",
      "Training:   0%|          | 0/123 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\trainer.py:80\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     78\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m     train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     val_loss, val_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ikurnosau\\Projects\\QuantitativeTrading\\intraday-portfolio-management\\modeling\\trainer.py:168\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler, OneCycleLR):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 168\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee085a5-1251-4816-84f2-f591c882b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectory_loader = TrajectoryDataset(X_train, next_return_train, spread_train, volatility_train, trajectory_length=16).as_dataloader(\n",
    "    batch_size=8, \n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_trajectory_loader = TrajectoryDataset(X_val, next_return_val, spread_val, volatility_val, trajectory_length=16).as_dataloader(\n",
    "    batch_size=8, \n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "test_trajectory_loader = TrajectoryDataset(X_test, next_return_test, spread_test, volatility_test, trajectory_length=16).as_dataloader(\n",
    "    batch_size=8, \n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041b148-50dc-4ca0-88cb-43761885bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnvironment(\n",
    "    reward_function=EstimatedReturnReward(fee=0.0, spread_multiplier=0.99),\n",
    ")\n",
    "\n",
    "backend = FullyConnectedBackend(\n",
    "    n_assets=len(config.data_config.symbol_or_symbols),\n",
    "    hidden_dim=128,\n",
    "    num_layers=2, \n",
    "    dropout=0.1,\n",
    "    use_layer_norm=False,\n",
    ")\n",
    "\n",
    "actor = RlActor(\n",
    "    model, \n",
    "    backend,\n",
    "    n_assets=len(config.data_config.symbol_or_symbols),\n",
    "    train_signal_predictor=False, \n",
    "    exploration_eps=0.0\n",
    ").to(device)\n",
    "\n",
    "signal_predictor_actor = SignalPredictorActor(\n",
    "    model, \n",
    "    trade_asset_count=1,\n",
    "    train_signal_predictor=False\n",
    ").to(device)\n",
    "\n",
    "rl_agent = RlAgent(\n",
    "    actor, \n",
    "    env,\n",
    "    single_action_per_trajectory=False\n",
    ")\n",
    "\n",
    "metrics_calculator = MetricsCalculator(\n",
    "    metrics=DEFAULT_METRICS\n",
    ")\n",
    "\n",
    "policy_gradient = PolicyGradient(\n",
    "    rl_agent, \n",
    "    train_trajectory_loader, \n",
    "    val_trajectory_loader, \n",
    "    metrics_calculator=metrics_calculator,\n",
    "    optimizer=torch.optim.AdamW(\n",
    "        [p for p in actor.parameters() if p.requires_grad], \n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        amsgrad=True),\n",
    "    scheduler=None,\n",
    "    loss_fn=SumLogReturnLoss(use_baseline=False),\n",
    "    num_epochs=10,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8dec5-eaf9-4f91-af35-30ea24c0e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 16:32:01,912 - INFO - [PolicyGradient] [VAL] Epoch 0/10 ‚Äî CumulativeReturn: 0.4364, MeanReturnPercentage: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PolicyGradient] [VAL] Epoch 0/10 ‚Äî Loss: -0.0453\n"
     ]
    }
   ],
   "source": [
    "print('Val set evaluation')\n",
    "epoch_loss, realized_returns_signal_predictor, actions_signal_predictor = policy_gradient.evaluate(signal_predictor_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set evaluation')\n",
    "epoch_loss, realized_returns_signal_predictor, actions_signal_predictor = policy_gradient.evaluate(signal_predictor_actor, test_trajectory_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c7dcead-f58a-4306-84ab-9ca83cfb332c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't subtract offset-naive and offset-aware datetimes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_cumulative_wealth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturns_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSignal Predictor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealized_returns_signal_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_set_last_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/intraday-portfolio-management/modeling/rl/visualization/wealth_plot.py:10\u001b[39m, in \u001b[36mplot_cumulative_wealth\u001b[39m\u001b[34m(returns_dict, start_time, end_time)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_cumulative_wealth\u001b[39m(returns_dict: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]], start_time: datetime, end_time: datetime):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Create uniform datetime range\u001b[39;00m\n\u001b[32m      8\u001b[39m     n_points = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(returns_dict.values())))  \u001b[38;5;66;03m# get length from any series\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     time_points = \u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Initial wealth\u001b[39;00m\n\u001b[32m     16\u001b[39m     initial_wealth = \u001b[32m1.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/intraday-portfolio-management/modeling/rl/visualization/wealth_plot.py:11\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_cumulative_wealth\u001b[39m(returns_dict: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]], start_time: datetime, end_time: datetime):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Create uniform datetime range\u001b[39;00m\n\u001b[32m      8\u001b[39m     n_points = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(returns_dict.values())))  \u001b[38;5;66;03m# get length from any series\u001b[39;00m\n\u001b[32m     10\u001b[39m     time_points = [\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         start_time + i * (\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m) / (n_points - \u001b[32m1\u001b[39m)\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_points)\n\u001b[32m     13\u001b[39m     ]\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Initial wealth\u001b[39;00m\n\u001b[32m     16\u001b[39m     initial_wealth = \u001b[32m1.0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: can't subtract offset-naive and offset-aware datetimes"
     ]
    }
   ],
   "source": [
    "plot_cumulative_wealth(\n",
    "    returns_dict={\n",
    "        'Signal Predictor': realized_returns_signal_predictor,\n",
    "    }, \n",
    "    start_time=config.data_config.train_set_last_date, \n",
    "    end_time=config.data_config.end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9955c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy  # Local import to avoid polluting global namespace unnecessarily\n",
    "state_dict = (\n",
    "    model.module.state_dict()\n",
    "        if isinstance(model, torch.nn.DataParallel)\n",
    "    else model.state_dict()\n",
    ")\n",
    "\n",
    "# Keep a local copy of the best weights so we can return the best model\n",
    "# after training finishes, without needing to reload from disk.\n",
    "best_model_state = copy.deepcopy(state_dict)\n",
    "\n",
    "# Persist to disk if a save_path was provided\n",
    "torch.save(state_dict, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed829636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/26 15:35:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'LSTM Default' already exists. Creating a new version of this model...\n",
      "2025/06/26 15:35:14 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LSTM Default, version 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run gentle-loon-699 at: http://127.0.0.1:8080/#/experiments/439216085822475480/runs/54deb1104660468d9ffb4e7e278e9cfb\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/439216085822475480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '10' of model 'LSTM Default'.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c286205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9435\n",
      "[LightGBM] [Info] Number of data points in the train set: 7371, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.497863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train rmse: 0.26411260601695974, Test rmse: 0.2684210886033184, Baseline rmse: 0.2599985897541046\n",
      "Expected return: 0.00010183148393891163, Baseline return: 2.569958041931386e-06, Max possible return 0.00048079571570269763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluate_lgb_regressor(X_train, y_train, X_val, y_val, next_return_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
