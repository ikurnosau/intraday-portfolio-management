{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a6dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime, timezone\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format for the log messages\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Log to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from data.raw.retrievers.alpaca_markets_retriever import AlpacaMarketsRetriever\n",
    "from config.constants import *\n",
    "from data.processed.dataset_creation import DatasetCreator\n",
    "from data.processed.indicators import *\n",
    "from data.processed.targets import Balanced3ClassClassification\n",
    "from data.processed.normalization import ZScoreOverWindowNormalizer, ZScoreNormalizer, MinMaxNormalizer\n",
    "from data.processed.dataset_pytorch import DatasetPytorch\n",
    "from modeling.trainer import Trainer\n",
    "from modeling.evaluate import evaluate_lgb_regressor, evaluate_torch_regressor, evaluate_torch_regressor_multiasset\n",
    "# from observability.mlflow_integration import log_experiment\n",
    "\n",
    "from modeling.rl.environment import PortfolioEnvironment\n",
    "from modeling.rl.state import State\n",
    "from modeling.rl.agent import RlAgent\n",
    "from modeling.rl.algorithms.policy_gradient import PolicyGradient\n",
    "from modeling.rl.actors.actor import RlActor, FullyConnectedBackend, TransformerBackend\n",
    "from modeling.rl.actors.signal_predictor_actor import SignalPredictorActor\n",
    "from modeling.rl.actors.high_energy_low_friction_actor import HighEnergyLowFrictionActor\n",
    "from modeling.rl.actors.xsmom_actor import XSMomActor\n",
    "from modeling.rl.actors.tsmom_actor import TSMomActor\n",
    "from modeling.rl.actors.blsw_actor import BLSWActor\n",
    "from modeling.rl.trajectory_dataset import TrajectoryDataset\n",
    "from modeling.rl.metrics import MetricsCalculator, DEFAULT_METRICS\n",
    "from modeling.rl.reward import EstimatedReturnReward\n",
    "from modeling.rl.loss import SumLogReturnLoss\n",
    "from modeling.rl.visualization.wealth_plot import plot_cumulative_wealth\n",
    "from modeling.rl.visualization.position_plot import plot_position_heatmap\n",
    "from config.experiments.cur_experiment import config\n",
    "\n",
    "torch.backends.cudnn.benchmark = config.train_config.cudnn_benchmark\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d63c4606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15Min'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(config.data_config.frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "755a729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = AlpacaMarketsRetriever(download_from_gdrive=False, timeframe=config.data_config.frequency)\n",
    "\n",
    "retrieval_result = retriever.bars_with_quotes(\n",
    "    symbol_or_symbols=config.data_config.symbol_or_symbols, \n",
    "    start=config.data_config.start, \n",
    "    end=config.data_config.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d48d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:42:10,694 - INFO - Processing AAPL ‚Ä¶\n",
      "2025-08-13 17:42:13,900 - INFO - Imputing 55 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:14,288 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:14,375 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:14,400 - INFO - Processing AMGN ‚Ä¶\n",
      "2025-08-13 17:42:17,931 - INFO - Imputing 1908 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:18,340 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:18,439 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:18,460 - INFO - Processing AMZN ‚Ä¶\n",
      "2025-08-13 17:42:21,931 - INFO - Imputing 143 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:22,338 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:22,432 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:22,454 - INFO - Processing AXP ‚Ä¶\n",
      "2025-08-13 17:42:26,010 - INFO - Imputing 1734 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:26,396 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:26,497 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:26,518 - INFO - Processing BA ‚Ä¶\n",
      "2025-08-13 17:42:30,112 - INFO - Imputing 457 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:30,498 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:30,595 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:30,624 - INFO - Processing CAT ‚Ä¶\n",
      "2025-08-13 17:42:34,446 - INFO - Imputing 1005 NaN rows out of 61101 with forward fill..\n",
      "2025-08-13 17:42:34,835 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:34,933 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:34,954 - INFO - Processing CRM ‚Ä¶\n",
      "2025-08-13 17:42:38,671 - INFO - Imputing 869 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:39,052 - INFO - Spread has 2 NaNs\n",
      "2025-08-13 17:42:39,161 - INFO - Imputing 72 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:39,185 - INFO - Processing CSCO ‚Ä¶\n",
      "2025-08-13 17:42:42,846 - INFO - Imputing 428 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:43,232 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:43,332 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:43,361 - INFO - Processing CVX ‚Ä¶\n",
      "2025-08-13 17:42:47,648 - INFO - Imputing 759 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:48,098 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:48,198 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:48,227 - INFO - Processing DIS ‚Ä¶\n",
      "2025-08-13 17:42:52,255 - INFO - Imputing 450 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:52,846 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:53,010 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:53,040 - INFO - Processing GS ‚Ä¶\n",
      "2025-08-13 17:42:58,673 - INFO - Imputing 999 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:42:59,259 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:42:59,416 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:42:59,448 - INFO - Processing HD ‚Ä¶\n",
      "2025-08-13 17:43:06,610 - INFO - Imputing 1187 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:43:07,592 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:43:07,763 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:07,795 - INFO - Processing HON ‚Ä¶\n",
      "2025-08-13 17:43:12,836 - INFO - Imputing 2270 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:43:13,430 - INFO - Spread has 4 NaNs\n",
      "2025-08-13 17:43:13,609 - INFO - Imputing 70 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:13,652 - INFO - Processing IBM ‚Ä¶\n",
      "2025-08-13 17:43:19,342 - INFO - Imputing 1026 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:43:19,924 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:43:20,084 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:20,117 - INFO - Processing JNJ ‚Ä¶\n",
      "2025-08-13 17:43:25,274 - INFO - Imputing 1169 NaN rows out of 61101 with forward fill..\n",
      "2025-08-13 17:43:25,694 - INFO - Spread has 1 NaNs\n",
      "2025-08-13 17:43:25,828 - INFO - Imputing 72 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:25,863 - INFO - Processing JPM ‚Ä¶\n",
      "2025-08-13 17:43:30,259 - INFO - Imputing 392 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:43:30,674 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:43:30,773 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:30,802 - INFO - Processing KO ‚Ä¶\n",
      "2025-08-13 17:43:34,942 - INFO - Imputing 634 NaN rows out of 61101 with forward fill..\n",
      "2025-08-13 17:43:35,366 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:43:35,481 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:35,511 - INFO - Processing MCD ‚Ä¶\n",
      "2025-08-13 17:43:39,554 - INFO - Imputing 1586 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:43:39,927 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:43:40,020 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:40,046 - INFO - Processing MMM ‚Ä¶\n",
      "2025-08-13 17:43:44,045 - INFO - Imputing 1796 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:43:44,421 - INFO - Spread has 2 NaNs\n",
      "2025-08-13 17:43:44,516 - INFO - Imputing 72 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:44,542 - INFO - Processing MRK ‚Ä¶\n",
      "2025-08-13 17:43:48,259 - INFO - Imputing 1205 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:43:48,631 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:43:48,726 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:48,747 - INFO - Processing MSFT ‚Ä¶\n",
      "2025-08-13 17:43:52,517 - INFO - Imputing 126 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:43:52,877 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:43:52,967 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:52,993 - INFO - Processing NKE ‚Ä¶\n",
      "2025-08-13 17:43:56,705 - INFO - Imputing 892 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:43:57,080 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:43:57,169 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:43:57,191 - INFO - Processing NVDA ‚Ä¶\n",
      "2025-08-13 17:44:01,063 - INFO - Imputing 151 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:44:01,431 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:44:01,526 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:44:01,545 - INFO - Processing PG ‚Ä¶\n",
      "2025-08-13 17:44:05,039 - INFO - Imputing 1500 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:44:05,400 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:44:05,517 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:44:05,546 - INFO - Processing SHW ‚Ä¶\n",
      "2025-08-13 17:44:09,465 - INFO - Imputing 3102 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:44:09,875 - INFO - Spread has 3 NaNs\n",
      "2025-08-13 17:44:09,974 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:44:10,003 - INFO - Processing TRV ‚Ä¶\n",
      "2025-08-13 17:44:15,107 - INFO - Imputing 2692 NaN rows out of 61101 with forward fill..\n",
      "2025-08-13 17:44:15,538 - INFO - Spread has 4 NaNs\n",
      "2025-08-13 17:44:15,643 - INFO - Imputing 70 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:44:15,672 - INFO - Processing UNH ‚Ä¶\n",
      "2025-08-13 17:44:19,937 - INFO - Imputing 1889 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:44:20,467 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:44:20,622 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:44:20,653 - INFO - Processing VZ ‚Ä¶\n",
      "2025-08-13 17:44:26,071 - INFO - Imputing 516 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:44:26,636 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:44:26,812 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:44:26,841 - INFO - Processing V ‚Ä¶\n",
      "2025-08-13 17:44:32,109 - INFO - Imputing 951 NaN rows out of 61101 with forward fill..\n",
      "2025-08-13 17:44:32,666 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:44:32,828 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:44:32,864 - INFO - Processing WMT ‚Ä¶\n",
      "2025-08-13 17:44:38,236 - INFO - Imputing 701 NaN rows out of 61128 with forward fill..\n",
      "2025-08-13 17:44:38,813 - INFO - Spread has 0 NaNs\n",
      "2025-08-13 17:44:38,977 - INFO - Imputing 71 NaN rows with 0.5 sentinel value\n",
      "2025-08-13 17:45:31,353 - INFO - Finished feature generation. Dropped 0 assets by length threshold. Kept 30 assets with 54310 aligned rows each. Max features len prior to alignment: 54336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((48203, 30, 60, 15),\n",
       " (48203, 30),\n",
       " (48203, 30),\n",
       " (48203, 30),\n",
       " (48203, 30),\n",
       " (5989, 30, 60, 15),\n",
       " (5989, 30),\n",
       " (5989, 30),\n",
       " (5989, 30),\n",
       " (5989, 30))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_creator = DatasetCreator(\n",
    "    features=config.data_config.features,\n",
    "    target=config.data_config.target,\n",
    "    normalizer=config.data_config.normalizer,\n",
    "    missing_values_handler=config.data_config.missing_values_handler,\n",
    "    train_set_last_date=config.data_config.train_set_last_date, \n",
    "    cutoff_time=config.data_config.cutoff_time,\n",
    "    in_seq_len=config.data_config.in_seq_len,\n",
    "    multi_asset_prediction=config.data_config.multi_asset_prediction,\n",
    ")\n",
    "\n",
    "X_train, y_train, next_return_train, spread_train, volatility_train, X_test, y_test, next_return_test, spread_test, volatility_test = dataset_creator.create_dataset_numpy(retrieval_result)\n",
    "X_train.shape, y_train.shape, next_return_train.shape, spread_train.shape, volatility_train.shape, X_test.shape, y_test.shape, next_return_test.shape, spread_test.shape, volatility_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d115f18-b628-43a2-bdee-01dd74f6bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, y_test, next_return_test, spread_test, volatility_test = X_test[:7000], y_test[:7000], next_return_test[:7000], spread_test[:7000], volatility_test[:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc91696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49983266, 0.50164175)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98a4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetPytorch(X_train, y_train, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")\n",
    "test_loader = DatasetPytorch(X_test, y_test, learning_task='regression').as_dataloader(\n",
    "    batch_size=config.train_config.batch_size,\n",
    "    shuffle=config.train_config.shuffle,\n",
    "    num_workers=config.train_config.num_workers,\n",
    "    prefetch_factor=config.train_config.prefetch_factor,\n",
    "    pin_memory=config.train_config.pin_memory,\n",
    "    persistent_workers=config.train_config.persistent_workers,\n",
    "    drop_last=config.train_config.drop_last\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4b5aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalSpatial(\n",
       "  (asset_embed): Embedding(50, 16)\n",
       "  (asset_proj): Linear(in_features=16, out_features=256, bias=False)\n",
       "  (lstm): LSTM(15, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (spatial_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = config.model_config.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa26ca6-8128-4e85-be01-34335ab51675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentConfig(data_config=DataConfig(symbol_or_symbols=['AAPL', 'AMD', 'BABA', 'BITU', 'C', 'CSCO', 'DAL', 'DIA', 'GLD', 'GOOG', 'IJR', 'MARA', 'MRVL', 'MU', 'NEE', 'NKE', 'NVDA', 'ON', 'PLTR', 'PYPL', 'QLD', 'QQQ', 'QQQM', 'RKLB', 'RSP', 'SMCI', 'SMH', 'SOXL', 'SOXX', 'SPXL', 'SPY', 'TMF', 'TNA', 'TQQQ', 'TSLA', 'UBER', 'UDOW', 'UPRO', 'VOO', 'WFC', 'XBI', 'XLC', 'XLE', 'XLI', 'XLK', 'XLU', 'XLV', 'XLY', 'XOM', 'XRT'], start=datetime.datetime(2024, 6, 1, 0, 0), end=datetime.datetime(2025, 6, 1, 0, 0), features={'log_ret': <function <lambda> at 0x7f96b70f6ca0>, 'hl_range': <function <lambda> at 0x7f96b70f6de0>, 'close_open': <function <lambda> at 0x7f96b6f08a40>, 'vol_delta': <function <lambda> at 0x7f96b6f08ae0>, 'EMA_fast': <data.processed.indicators.EMA object at 0x7f96b70bda90>, 'EMA_slow': <data.processed.indicators.EMA object at 0x7f96b709cb90>, 'RSI2': <data.processed.indicators.RSI object at 0x7f96b704ff50>, 'RSI6': <data.processed.indicators.RSI object at 0x7f96c720af90>, 'realvol20': <function <lambda> at 0x7f96b6f08b80>, 'VWAP_dist': <function <lambda> at 0x7f96b6f08c20>, 'loc_in_range': <function <lambda> at 0x7f96b6f08cc0>, 'tod_sin': <function <lambda> at 0x7f96b6f08d60>, 'tod_cos': <function <lambda> at 0x7f96b6f08e00>, 'ema_slope': <function <lambda> at 0x7f96b6f08ea0>, 'vol_slope': <function <lambda> at 0x7f96b6f08f40>}, target=<data.processed.targets.FutureMeanReturnClassification object at 0x7f96b70f2d50>, normalizer=<data.processed.normalization.MinMaxNormalizerOverWindow object at 0x7f96b70d5450>, missing_values_handler=<data.processed.missing_values_handling.ForwardFillFlatBars object at 0x7f96be58cb10>, in_seq_len=60, train_set_last_date=datetime.datetime(2025, 5, 1, 0, 0, tzinfo=datetime.timezone.utc), multi_asset_prediction=True, cutoff_time=datetime.time(14, 10)), model_config=ModelConfig(model=TemporalSpatial(\n",
       "  (asset_embed): Embedding(50, 16)\n",
       "  (asset_proj): Linear(in_features=16, out_features=256, bias=False)\n",
       "  (lstm): LSTM(15, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (spatial_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "), registered_model_name='TemporalSpatial Regressor'), train_config=TrainConfig(loss_fn=MSELoss(), optimizer=AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: True\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 1e-10\n",
       "), scheduler={'type': 'OneCycleLR', 'max_lr': 0.003, 'pct_start': 0.1, 'div_factor': 25, 'final_div_factor': 1000.0, 'anneal_strategy': 'cos', 'cycle_momentum': False}, num_epochs=20, early_stopping_patience=5, device=device(type='cuda'), cudnn_benchmark=True, metrics={'rmse': <function rmse_regression at 0x7f96b6f080e0>}, batch_size=128, shuffle=True, num_workers=8, prefetch_factor=4, pin_memory=True, persistent_workers=True, drop_last=True, save_path=''), observability_config=ObservabilityConfig(experiment_name='Return Regression MLP'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3858fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    loss_fn=config.train_config.loss_fn,\n",
    "    optimizer=config.train_config.optimizer,\n",
    "    scheduler=config.train_config.scheduler,\n",
    "    num_epochs=config.train_config.num_epochs,\n",
    "    device=config.train_config.device,\n",
    "    metrics=config.train_config.metrics,\n",
    "    save_path=config.train_config.save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6d315-3bf8-452a-ba17-6badd562dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1154\n",
    "# 0.3397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01c52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 16:14:37,973 - INFO - Epoch 1/20\n",
      "2025-08-11 16:15:24,477 - INFO - Train Loss: 0.1358        \n",
      "2025-08-11 16:15:24,477 - INFO - Train Rmse: 0.3670\n",
      "2025-08-11 16:15:24,477 - INFO - Val   Loss: 0.1219\n",
      "2025-08-11 16:15:24,477 - INFO - Val   Rmse: 0.3490\n",
      "2025-08-11 16:15:24,477 - INFO - \n",
      "2025-08-11 16:15:24,479 - INFO - Epoch 2/20\n",
      "2025-08-11 16:16:05,813 - INFO - Train Loss: 0.1253        \n",
      "2025-08-11 16:16:05,814 - INFO - Train Rmse: 0.3539\n",
      "2025-08-11 16:16:05,814 - INFO - Val   Loss: 0.1171\n",
      "2025-08-11 16:16:05,814 - INFO - Val   Rmse: 0.3421\n",
      "2025-08-11 16:16:05,815 - INFO - \n",
      "2025-08-11 16:16:05,816 - INFO - Epoch 3/20\n",
      "2025-08-11 16:16:47,121 - INFO - Train Loss: 0.1240        \n",
      "2025-08-11 16:16:47,121 - INFO - Train Rmse: 0.3521\n",
      "2025-08-11 16:16:47,122 - INFO - Val   Loss: 0.1160\n",
      "2025-08-11 16:16:47,122 - INFO - Val   Rmse: 0.3405\n",
      "2025-08-11 16:16:47,122 - INFO - \n",
      "2025-08-11 16:16:47,123 - INFO - Epoch 4/20\n",
      "2025-08-11 16:17:28,418 - INFO - Train Loss: 0.1235        \n",
      "2025-08-11 16:17:28,419 - INFO - Train Rmse: 0.3514\n",
      "2025-08-11 16:17:28,419 - INFO - Val   Loss: 0.1161\n",
      "2025-08-11 16:17:28,419 - INFO - Val   Rmse: 0.3408\n",
      "2025-08-11 16:17:28,419 - INFO - \n",
      "2025-08-11 16:17:28,419 - INFO - Epoch 5/20\n",
      "2025-08-11 16:18:09,781 - INFO - Train Loss: 0.1231        \n",
      "2025-08-11 16:18:09,782 - INFO - Train Rmse: 0.3507\n",
      "2025-08-11 16:18:09,782 - INFO - Val   Loss: 0.1160\n",
      "2025-08-11 16:18:09,782 - INFO - Val   Rmse: 0.3406\n",
      "2025-08-11 16:18:09,782 - INFO - \n",
      "2025-08-11 16:18:09,782 - INFO - Epoch 6/20\n",
      "2025-08-11 16:18:51,146 - INFO - Train Loss: 0.1228        \n",
      "2025-08-11 16:18:51,146 - INFO - Train Rmse: 0.3504\n",
      "2025-08-11 16:18:51,146 - INFO - Val   Loss: 0.1157\n",
      "2025-08-11 16:18:51,146 - INFO - Val   Rmse: 0.3402\n",
      "2025-08-11 16:18:51,146 - INFO - \n",
      "2025-08-11 16:18:51,148 - INFO - Epoch 7/20\n",
      "2025-08-11 16:19:32,458 - INFO - Train Loss: 0.1226        \n",
      "2025-08-11 16:19:32,459 - INFO - Train Rmse: 0.3501\n",
      "2025-08-11 16:19:32,459 - INFO - Val   Loss: 0.1158\n",
      "2025-08-11 16:19:32,459 - INFO - Val   Rmse: 0.3402\n",
      "2025-08-11 16:19:32,459 - INFO - \n",
      "2025-08-11 16:19:32,459 - INFO - Epoch 8/20\n",
      "2025-08-11 16:20:13,769 - INFO - Train Loss: 0.1225        \n",
      "2025-08-11 16:20:13,770 - INFO - Train Rmse: 0.3500\n",
      "2025-08-11 16:20:13,770 - INFO - Val   Loss: 0.1156\n",
      "2025-08-11 16:20:13,770 - INFO - Val   Rmse: 0.3400\n",
      "2025-08-11 16:20:13,770 - INFO - \n",
      "2025-08-11 16:20:13,772 - INFO - Epoch 9/20\n",
      "2025-08-11 16:20:55,099 - INFO - Train Loss: 0.1225        \n",
      "2025-08-11 16:20:55,100 - INFO - Train Rmse: 0.3499\n",
      "2025-08-11 16:20:55,100 - INFO - Val   Loss: 0.1157\n",
      "2025-08-11 16:20:55,100 - INFO - Val   Rmse: 0.3401\n",
      "2025-08-11 16:20:55,100 - INFO - \n",
      "2025-08-11 16:20:55,100 - INFO - Epoch 10/20\n",
      "2025-08-11 16:21:36,464 - INFO - Train Loss: 0.1224        \n",
      "2025-08-11 16:21:36,464 - INFO - Train Rmse: 0.3498\n",
      "2025-08-11 16:21:36,464 - INFO - Val   Loss: 0.1156\n",
      "2025-08-11 16:21:36,465 - INFO - Val   Rmse: 0.3400\n",
      "2025-08-11 16:21:36,465 - INFO - \n",
      "2025-08-11 16:21:36,466 - INFO - Epoch 11/20\n",
      "2025-08-11 16:22:17,815 - INFO - Train Loss: 0.1224        \n",
      "2025-08-11 16:22:17,816 - INFO - Train Rmse: 0.3497\n",
      "2025-08-11 16:22:17,816 - INFO - Val   Loss: 0.1157\n",
      "2025-08-11 16:22:17,816 - INFO - Val   Rmse: 0.3400\n",
      "2025-08-11 16:22:17,816 - INFO - \n",
      "2025-08-11 16:22:17,816 - INFO - Epoch 12/20\n",
      "2025-08-11 16:22:59,138 - INFO - Train Loss: 0.1223        \n",
      "2025-08-11 16:22:59,139 - INFO - Train Rmse: 0.3496\n",
      "2025-08-11 16:22:59,139 - INFO - Val   Loss: 0.1154\n",
      "2025-08-11 16:22:59,139 - INFO - Val   Rmse: 0.3397\n",
      "2025-08-11 16:22:59,139 - INFO - \n",
      "2025-08-11 16:22:59,141 - INFO - Epoch 13/20\n",
      "2025-08-11 16:23:40,530 - INFO - Train Loss: 0.1222        \n",
      "2025-08-11 16:23:40,531 - INFO - Train Rmse: 0.3496\n",
      "2025-08-11 16:23:40,531 - INFO - Val   Loss: 0.1156\n",
      "2025-08-11 16:23:40,531 - INFO - Val   Rmse: 0.3399\n",
      "2025-08-11 16:23:40,531 - INFO - \n",
      "2025-08-11 16:23:40,531 - INFO - Epoch 14/20\n",
      "2025-08-11 16:24:21,861 - INFO - Train Loss: 0.1222        \n",
      "2025-08-11 16:24:21,862 - INFO - Train Rmse: 0.3495\n",
      "2025-08-11 16:24:21,862 - INFO - Val   Loss: 0.1155\n",
      "2025-08-11 16:24:21,862 - INFO - Val   Rmse: 0.3398\n",
      "2025-08-11 16:24:21,862 - INFO - \n",
      "2025-08-11 16:24:21,862 - INFO - Epoch 15/20\n",
      "2025-08-11 16:25:03,188 - INFO - Train Loss: 0.1222        \n",
      "2025-08-11 16:25:03,189 - INFO - Train Rmse: 0.3495\n",
      "2025-08-11 16:25:03,189 - INFO - Val   Loss: 0.1156\n",
      "2025-08-11 16:25:03,190 - INFO - Val   Rmse: 0.3399\n",
      "2025-08-11 16:25:03,190 - INFO - \n",
      "2025-08-11 16:25:03,190 - INFO - Epoch 16/20\n",
      "2025-08-11 16:25:44,498 - INFO - Train Loss: 0.1221        \n",
      "2025-08-11 16:25:44,498 - INFO - Train Rmse: 0.3494\n",
      "2025-08-11 16:25:44,498 - INFO - Val   Loss: 0.1154\n",
      "2025-08-11 16:25:44,498 - INFO - Val   Rmse: 0.3397\n",
      "2025-08-11 16:25:44,499 - INFO - \n",
      "2025-08-11 16:25:44,499 - INFO - Epoch 17/20\n",
      "2025-08-11 16:26:25,791 - INFO - Train Loss: 0.1221        \n",
      "2025-08-11 16:26:25,791 - INFO - Train Rmse: 0.3494\n",
      "2025-08-11 16:26:25,792 - INFO - Val   Loss: 0.1155\n",
      "2025-08-11 16:26:25,792 - INFO - Val   Rmse: 0.3398\n",
      "2025-08-11 16:26:25,792 - INFO - \n",
      "2025-08-11 16:26:25,792 - INFO - Early stopping triggered at epoch 17\n"
     ]
    }
   ],
   "source": [
    "model, history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ee085a5-1251-4816-84f2-f591c882b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectory_loader = TrajectoryDataset(X_train, next_return_train, spread_train, volatility_train, trajectory_length=16).as_dataloader(\n",
    "    batch_size=8, \n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_trajectory_loader = TrajectoryDataset(X_test, next_return_test, spread_test, volatility_test, trajectory_length=16).as_dataloader(\n",
    "    batch_size=8, \n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e041b148-50dc-4ca0-88cb-43761885bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnvironment(\n",
    "    reward_function=EstimatedReturnReward(fee=0.0, spread_multiplier=0.99),\n",
    ")\n",
    "\n",
    "backend = FullyConnectedBackend(\n",
    "    n_assets=len(config.data_config.symbol_or_symbols),\n",
    "    hidden_dim=128,\n",
    "    num_layers=2, \n",
    "    dropout=0.1,\n",
    "    use_layer_norm=False,\n",
    ")\n",
    "\n",
    "actor = RlActor(\n",
    "    model, \n",
    "    backend,\n",
    "    n_assets=len(config.data_config.symbol_or_symbols),\n",
    "    train_signal_predictor=False, \n",
    "    exploration_eps=0.0\n",
    ").to(device)\n",
    "\n",
    "signal_predictor_actor = SignalPredictorActor(\n",
    "    model, \n",
    "    trade_asset_count=1,\n",
    "    train_signal_predictor=False\n",
    ").to(device)\n",
    "\n",
    "rl_agent = RlAgent(\n",
    "    actor, \n",
    "    env,\n",
    "    single_action_per_trajectory=False\n",
    ")\n",
    "\n",
    "metrics_calculator = MetricsCalculator(\n",
    "    metrics=DEFAULT_METRICS\n",
    ")\n",
    "\n",
    "policy_gradient = PolicyGradient(\n",
    "    rl_agent, \n",
    "    train_trajectory_loader, \n",
    "    val_trajectory_loader, \n",
    "    metrics_calculator=metrics_calculator,\n",
    "    optimizer=torch.optim.AdamW(\n",
    "        [p for p in actor.parameters() if p.requires_grad], \n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        amsgrad=True),\n",
    "    scheduler=None,\n",
    "    loss_fn=SumLogReturnLoss(use_baseline=False),\n",
    "    num_epochs=10,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fa8dec5-eaf9-4f91-af35-30ea24c0e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 16:32:01,912 - INFO - [PolicyGradient] [VAL] Epoch 0/10 ‚Äî CumulativeReturn: 0.4364, MeanReturnPercentage: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PolicyGradient] [VAL] Epoch 0/10 ‚Äî Loss: -0.0453\n"
     ]
    }
   ],
   "source": [
    "epoch_loss, realized_returns_signal_predictor, actions_signal_predictor = policy_gradient.evaluate(signal_predictor_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c7dcead-f58a-4306-84ab-9ca83cfb332c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't subtract offset-naive and offset-aware datetimes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_cumulative_wealth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturns_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSignal Predictor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealized_returns_signal_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_set_last_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/intraday-portfolio-management/modeling/rl/visualization/wealth_plot.py:10\u001b[39m, in \u001b[36mplot_cumulative_wealth\u001b[39m\u001b[34m(returns_dict, start_time, end_time)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_cumulative_wealth\u001b[39m(returns_dict: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]], start_time: datetime, end_time: datetime):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Create uniform datetime range\u001b[39;00m\n\u001b[32m      8\u001b[39m     n_points = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(returns_dict.values())))  \u001b[38;5;66;03m# get length from any series\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     time_points = \u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Initial wealth\u001b[39;00m\n\u001b[32m     16\u001b[39m     initial_wealth = \u001b[32m1.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/intraday-portfolio-management/modeling/rl/visualization/wealth_plot.py:11\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_cumulative_wealth\u001b[39m(returns_dict: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]], start_time: datetime, end_time: datetime):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Create uniform datetime range\u001b[39;00m\n\u001b[32m      8\u001b[39m     n_points = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(returns_dict.values())))  \u001b[38;5;66;03m# get length from any series\u001b[39;00m\n\u001b[32m     10\u001b[39m     time_points = [\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         start_time + i * (\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m) / (n_points - \u001b[32m1\u001b[39m)\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_points)\n\u001b[32m     13\u001b[39m     ]\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Initial wealth\u001b[39;00m\n\u001b[32m     16\u001b[39m     initial_wealth = \u001b[32m1.0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: can't subtract offset-naive and offset-aware datetimes"
     ]
    }
   ],
   "source": [
    "plot_cumulative_wealth(\n",
    "    returns_dict={\n",
    "        'Signal Predictor': realized_returns_signal_predictor,\n",
    "    }, \n",
    "    start_time=config.data_config.train_set_last_date, \n",
    "    end_time=config.data_config.end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9955c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy  # Local import to avoid polluting global namespace unnecessarily\n",
    "state_dict = (\n",
    "    model.module.state_dict()\n",
    "        if isinstance(model, torch.nn.DataParallel)\n",
    "    else model.state_dict()\n",
    ")\n",
    "\n",
    "# Keep a local copy of the best weights so we can return the best model\n",
    "# after training finishes, without needing to reload from disk.\n",
    "best_model_state = copy.deepcopy(state_dict)\n",
    "\n",
    "# Persist to disk if a save_path was provided\n",
    "torch.save(state_dict, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed829636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/26 15:35:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'LSTM Default' already exists. Creating a new version of this model...\n",
      "2025/06/26 15:35:14 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LSTM Default, version 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run gentle-loon-699 at: http://127.0.0.1:8080/#/experiments/439216085822475480/runs/54deb1104660468d9ffb4e7e278e9cfb\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/439216085822475480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '10' of model 'LSTM Default'.\n"
     ]
    }
   ],
   "source": [
    "log_experiment(\n",
    "    config=config, \n",
    "    model=model, \n",
    "    history=history,\n",
    "    input_data_sample=next(iter(train_loader))[0].to(trainer.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c286205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9435\n",
      "[LightGBM] [Info] Number of data points in the train set: 7371, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.497863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train rmse: 0.26411260601695974, Test rmse: 0.2684210886033184, Baseline rmse: 0.2599985897541046\n",
      "Expected return: 0.00010183148393891163, Baseline return: 2.569958041931386e-06, Max possible return 0.00048079571570269763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ikurnosau\\anaconda3\\envs\\mldl\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluate_lgb_regressor(X_train, y_train, X_test, y_test, next_return_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
